{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1avDQgkq-5"
   },
   "source": [
    "# CS 6140 Machine Learning: Assignment - 1 (Total Points: 100)\n",
    "## Prof. Ahmad Uzair "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFpvDelkq-6"
   },
   "source": [
    "### Q1. Decision Tree Classifier (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Growing Decison Trees from scratch (40 points)\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "You must also print the Decision Tree. Use information gain based on entropy as the splitting measure.\n",
    "\n",
    "Use the data.csv dataset for this particular question. The dataset should be uploaded on Canvas with Assignment 1. Split the dataset into training and test data and calculate testing accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utils class to perform certain calculations\n",
    "class Utils:\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)\n",
    "\n",
    "# Node of a DecisionTree. Can be either regular node or leaf node.\n",
    "# Normal node contains information about the feature and the value it compares in that node.\n",
    "# Leaf node contains the type of class.\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, X, Y):\n",
    "        if len(X) == 0:\n",
    "            return\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.isLeaf = False\n",
    "        self.classType = -1\n",
    "        self.H = self.entropy(Y)\n",
    "        self.trueChild = None\n",
    "        self.falseChild = None\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        for i in range(len(freq)):\n",
    "            if freq[i] == len(Y):\n",
    "                self.isLeaf = True\n",
    "                self.classType = num[i]\n",
    "                return\n",
    "\n",
    "        self.featureIndex, self.compValue = self.findBestSplit()\n",
    "        tx, ty, fx, fy = self.split(X, Y, self.featureIndex, self.compValue)\n",
    "        self.trueChild = DecisionTreeNode(tx, ty)\n",
    "        self.falseChild = DecisionTreeNode(fx, fy)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        h = 0.0\n",
    "        for val in freq:\n",
    "            if val != 0:\n",
    "                prob = val / len(Y)\n",
    "                h -= prob * (np.log2(prob))\n",
    "        return h\n",
    "\n",
    "    def informationGain(self, X, Y, featureIndex, value):\n",
    "        tx, ty, fx, fy = self.split(X, Y, featureIndex, value)\n",
    "        expectedEntropy = 0\n",
    "        expectedEntropy += (len(ty) / len(Y)) * self.entropy(ty)\n",
    "        expectedEntropy += (len(fy) / len(Y)) * self.entropy(fy)\n",
    "        IG = self.H - expectedEntropy\n",
    "        return IG\n",
    "\n",
    "    def split(self, X, Y, featureIndex, value):\n",
    "        tx, ty, fx, fy = [], [], [], []\n",
    "        for i in range(0, len(X)):\n",
    "            if X[i][featureIndex] < value:\n",
    "                tx.append(X[i])\n",
    "                ty.append(Y[i])\n",
    "            else:\n",
    "                fx.append(X[i])\n",
    "                fy.append(Y[i])\n",
    "        return np.array(tx), np.array(ty), np.array(fx), np.array(fy)\n",
    "\n",
    "    def findBestSplit(self):\n",
    "        copy_X = np.transpose(self.X)\n",
    "        maxIG = float(\"-inf\")\n",
    "        bestFeatureIndex = None\n",
    "        bestValue = None\n",
    "        for i in range(0, len(copy_X)):\n",
    "            T = np.sort(copy_X[i])\n",
    "            for j in range(1, len(T)):\n",
    "                midValue = (T[j - 1] + T[j]) / 2.0\n",
    "                currentIG = self.informationGain(self.X, self.Y, i, midValue)\n",
    "                if currentIG > maxIG:\n",
    "                    maxIG = currentIG\n",
    "                    bestFeatureIndex = i\n",
    "                    bestValue = midValue\n",
    "        return bestFeatureIndex, bestValue\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.isLeaf:\n",
    "            return self.classType\n",
    "        elif X[self.featureIndex] <= self.compValue:\n",
    "            return self.trueChild.predict(X)\n",
    "        else:\n",
    "            return self.falseChild.predict(X)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.isLeaf:\n",
    "            return \"class:\" + str(self.classType)\n",
    "        else:\n",
    "            return \"feature\" + str(self.featureIndex + 1) + \" <= \" + str(self.compValue)\n",
    "\n",
    "#The Decision Tree Class\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.root = DecisionTreeNode(X, Y)\n",
    "        return\n",
    "\n",
    "    def print(self, feature_names, class_names):\n",
    "        self.preOrder(self.root, feature_names, class_names, \"|--- \")\n",
    "\n",
    "    def preOrder(self, root, feature_names, class_names, prev):\n",
    "        if root == None:\n",
    "            return\n",
    "        if root.isLeaf:\n",
    "            print(prev + \"class: \" + class_names[root.classType])\n",
    "            return\n",
    "        print(prev + feature_names[root.featureIndex] + \" <= \" + str(root.compValue))\n",
    "        self.preOrder(root.trueChild, feature_names, class_names, \"|   \" + prev)\n",
    "        print(prev + feature_names[root.featureIndex] + \" >  \" + str(root.compValue))\n",
    "        self.preOrder(root.falseChild, feature_names, class_names, \"|   \" + prev)\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y = []\n",
    "        for i in range(len(X)):\n",
    "            Y.append(self.root.predict(X[i]))\n",
    "        return np.array(Y)\n",
    "\n",
    "    def accuracy(self, Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature4 <= 1.75\n",
      "|   |   |--- feature3 <= 4.95\n",
      "|   |   |   |--- feature4 <= 1.65\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature4 >  1.65\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature3 >  4.95\n",
      "|   |   |   |--- feature4 <= 1.55\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature4 >  1.55\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |--- feature4 >  1.75\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature1 <= 5.95\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  5.95\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "Testing Accuracy: 97.37\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_csv(\"data.csv\")\n",
    "X = data.drop(\"class\", axis=1).values\n",
    "Y = data[\"class\"].values\n",
    "\n",
    "feature_names = list(data.drop(\"class\", axis=1).columns)\n",
    "class_names = [str(i) for i in range(0, len(set(Y)))]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "dt.print(feature_names, class_names)\n",
    "\n",
    "Y_test_pred = dt.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Decision Tree using Sklearn Library (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Decision Tree Classifier from the Sklearn Library and use gini index as a splitting measure. Use the data.csv dataset.\n",
    "Calculate accuracy for this model. \n",
    "Print the Decision tree and compare the Decision Trees generated from your code and Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature4 <= 1.75\n",
      "|   |   |--- feature3 <= 4.95\n",
      "|   |   |   |--- feature4 <= 1.65\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature4 >  1.65\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature3 >  4.95\n",
      "|   |   |   |--- feature4 <= 1.55\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature4 >  1.55\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |--- feature4 >  1.75\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature1 <= 5.95\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  5.95\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "\n",
      "Testing Accuracy: 97.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(export_text(clf, feature_names=feature_names))\n",
    "\n",
    "Y_test_pred = clf.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Comparision of the two generated decision trees:\n",
    "\n",
    "Both the trees generated are almost identical most of the time.\n",
    "The difference in the trees could occur because of the different splitting measure (Information Gain for my code and Gini for sklearn code). The difference is still minimal and the accuracy achieved by both of the trees is similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEBH56pukq_H"
   },
   "source": [
    "### Q2 Linear Regression (40 points)\n",
    "\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FL1tuQEkq_H"
   },
   "source": [
    "## Gradient descent algorithm \n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This minimizes the following cost function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "h(x_i) = \\theta^T \\bar{x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "cOem1EjQkq_H"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "true_slope = 15\n",
    "true_intercept = 2.4\n",
    "input_var = np.arange(0.0,100.0)\n",
    "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1630902228487,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "SNvDqYEykq_H",
    "outputId": "c7f53823-73d9-473f-9e28-944f1f09a415",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.scatter(input_var, output_var)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "XgNhbpEmkq_I"
   },
   "outputs": [],
   "source": [
    "def compute_cost(ip, op, params):\n",
    "    \"\"\"\n",
    "    Cost function in linear regression where the cost is calculated\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    num_samples = len(ip)\n",
    "    cost_sum = 0.0\n",
    "    for x,y in zip(ip, op):\n",
    "        y_hat = np.dot(params, np.array([1.0, x]))\n",
    "        cost_sum += (y_hat - y) ** 2\n",
    "    \n",
    "    cost = cost_sum / (num_samples)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Implement Linear Regression using Batch Gradient Descent from scratch.  (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ao7aYu9kq_I"
   },
   "source": [
    "\n",
    "### Batch gradient descent\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "f0z876gDkq_I"
   },
   "outputs": [],
   "source": [
    "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(max_iter)\n",
    "    params_store = np.zeros([2, max_iter])\n",
    "\n",
    "    x = np.ones([num_samples, len(params)])\n",
    "    for i in range(num_samples):\n",
    "        x[i][1] = ip[i]\n",
    "    \n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    while iteration < max_iter:\n",
    "        cost[iteration] = compute_cost(ip, op, params)\n",
    "        params_store[:, iteration] = params\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {iteration}')\n",
    "        print(f'cost: {cost[iteration]}')\n",
    "\n",
    "\n",
    "        \n",
    "        # Apply batch gradient descent\n",
    "        for i in range(0, num_samples):\n",
    "            for j in range(len(params)):\n",
    "                prevParams = params\n",
    "                params[j] += (alpha / num_samples) * (op[i] - np.dot(prevParams, x[i])) * x[i][j]\n",
    "            # params[1] += (alpha / num_samples) * (op[i] - np.dot(prevParams, [1.0, ip[i]])) * ip[i]\n",
    "        iteration += 1\n",
    "    \n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "qbjhyZ71kq_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 12791555.49167471\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 33428.02382637368\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 14832.883421273587\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 14734.036580907477\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 14727.193659607705\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 14722.924581828567\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 14718.74960697125\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 14714.580145544576\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 14710.412997896628\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 14706.248047347359\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 14702.085288571378\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 14697.92472026592\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 14693.766341274142\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 14689.610150445038\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 14685.456146628381\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 14681.304328674585\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 14677.154695434627\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 14673.007245760129\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 14668.861978503282\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 14664.718892516914\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 14660.577986654422\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 14656.439259769837\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 14652.302710717788\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 14648.168338353435\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 14644.036141532657\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 14639.906119111853\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 14635.778269948056\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 14631.652592898881\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 14627.529086822553\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 14623.407750577879\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 14619.288583024321\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 14615.171583021869\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 14611.056749431149\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 14606.944081113386\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 14602.8335769304\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 14598.725235744578\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 14594.61905641898\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 14590.515037817182\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 14586.413178803408\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 14582.313478242455\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 14578.215934999724\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 14574.120547941213\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 14570.027315933523\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 14565.936237843849\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 14561.847312539943\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 14557.760538890196\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 14553.675915763593\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 14549.593442029693\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 14545.513116558646\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 14541.43493822123\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 14537.358905888777\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 14533.285018433231\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 14529.213274727106\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 14525.14367364356\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 14521.076214056313\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 14517.010894839626\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 14512.947714868462\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 14508.886673018249\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 14504.82776816512\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 14500.770999185741\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 14496.716364957345\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 14492.663864357819\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 14488.613496265583\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 14484.565259559682\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 14480.519153119738\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 14476.475175825946\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 14472.433326559127\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 14468.393604200633\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 14464.356007632474\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 14460.320535737188\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 14456.287187397922\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 14452.255961498426\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 14448.226856923018\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 14444.199872556597\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 14440.175007284644\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 14436.152259993269\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 14432.131629569116\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 14428.113114899443\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 14424.096714872085\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 14420.08242837543\n",
      "--------------------------\n",
      "iteration: 80\n",
      "cost: 14416.070254298518\n",
      "--------------------------\n",
      "iteration: 81\n",
      "cost: 14412.060191530918\n",
      "--------------------------\n",
      "iteration: 82\n",
      "cost: 14408.052238962806\n",
      "--------------------------\n",
      "iteration: 83\n",
      "cost: 14404.046395484915\n",
      "--------------------------\n",
      "iteration: 84\n",
      "cost: 14400.042659988609\n",
      "--------------------------\n",
      "iteration: 85\n",
      "cost: 14396.041031365783\n",
      "--------------------------\n",
      "iteration: 86\n",
      "cost: 14392.041508508939\n",
      "--------------------------\n",
      "iteration: 87\n",
      "cost: 14388.04409031116\n",
      "--------------------------\n",
      "iteration: 88\n",
      "cost: 14384.048775666122\n",
      "--------------------------\n",
      "iteration: 89\n",
      "cost: 14380.05556346804\n",
      "--------------------------\n",
      "iteration: 90\n",
      "cost: 14376.06445261178\n",
      "--------------------------\n",
      "iteration: 91\n",
      "cost: 14372.075441992705\n",
      "--------------------------\n",
      "iteration: 92\n",
      "cost: 14368.088530506819\n",
      "--------------------------\n",
      "iteration: 93\n",
      "cost: 14364.103717050686\n",
      "--------------------------\n",
      "iteration: 94\n",
      "cost: 14360.121000521478\n",
      "--------------------------\n",
      "iteration: 95\n",
      "cost: 14356.14037981689\n",
      "--------------------------\n",
      "iteration: 96\n",
      "cost: 14352.16185383522\n",
      "--------------------------\n",
      "iteration: 97\n",
      "cost: 14348.185421475364\n",
      "--------------------------\n",
      "iteration: 98\n",
      "cost: 14344.211081636784\n",
      "--------------------------\n",
      "iteration: 99\n",
      "cost: 14340.238833219535\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "# Training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
    "\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "\n",
    "alpha_batch = 1e-3\n",
    "max_iter = 100\n",
    "params_hat_batch, cost_batch, params_store_batch =\\\n",
    "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Implement Stochastic Gradient Descent from scratch. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEIJL-WGkq_I"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "Algorithm can be given as follows:\n",
    "```shuffle(x, y)\n",
    "for i in 0 -> m:\n",
    "    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "Gx9LN0wQkq_I"
   },
   "outputs": [],
   "source": [
    "# I changed input_var, output_var to ip, op as it was not taking the parameter values but taking the input_var and output_var (the whole data) defined in the earlier block.\n",
    "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using stochastic gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(num_samples)\n",
    "    params_store = np.zeros([2, num_samples])\n",
    "\n",
    "    i = 0\n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    for x, y in zip(ip, op):\n",
    "        cost[i] = compute_cost(ip, op, params)\n",
    "        params_store[:, i] = params\n",
    "\n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {i}')\n",
    "        print(f'cost: {cost[i]}')\n",
    "\n",
    "        # Apply stochastic gradient descent\n",
    "        X = [1.0, x]\n",
    "        for j in range(len(params)):\n",
    "            prevParams = params\n",
    "            params[j] += (alpha / num_samples) * (y - np.dot(prevParams, X)) * X[j]\n",
    "        i+=1\n",
    "\n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "HivE1gVkkq_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 12791555.49167471\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 10176540.036424775\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 9916473.426795237\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 9674192.482093198\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 8940887.060511801\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 7739334.921925751\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 6594603.299423821\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 6582169.771992964\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 6214370.743814383\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 5538248.748048183\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 4975790.852330693\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 4607736.983019125\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 4178062.247624268\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 3344684.9489454497\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 2585104.4585321276\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 2032783.9847029396\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 2023100.5143183675\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 1819365.2472741264\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 1726689.304379264\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 1652660.6452977227\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 1382299.5983239454\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 1336038.1978859515\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 1304139.1176421705\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 1237863.7638903582\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 1231354.3585951296\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 1105260.7808668632\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 1071527.082013046\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 931071.5222316424\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 900864.5984037586\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 839335.0721561\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 839389.335270166\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 753993.2882097889\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 737256.8038275242\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 634450.056321854\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 592502.974195625\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 593161.7421613085\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 491732.1736083751\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 437646.65032718575\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 379700.60669748776\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 378689.5370236589\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 374220.4200622495\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 305650.0361050932\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 234968.52525787405\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 235791.97126834496\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 233142.04097168782\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 221836.84580822955\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 204604.5963071606\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 158722.88632369626\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 142430.90825638743\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 140932.34481149534\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 141248.96830159504\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 136219.88302024602\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 136506.41554088442\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 135235.19554011733\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 127560.51242589676\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 128003.18234834098\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 126629.90879148715\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 127141.14225471971\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 125055.50616486785\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 125465.91099849914\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 103238.77249807901\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 104045.22669272749\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 82258.39552786024\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 77898.77682258314\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 69789.57425137659\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 70598.9255041367\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 63000.1449573982\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 60092.808704931405\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 55109.99849764081\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 55400.04719717281\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 54563.52693408072\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 54866.78809639304\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 55039.1067098905\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 54281.000694701914\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 54281.525535437904\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 55019.78289194578\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 49162.23168786597\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 49043.52789284003\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 48918.724055036095\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 38766.73478538944\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "alpha = 1e-3\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "params_hat, cost, params_store =\\\n",
    "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error for Batch Gradient Descent: 94.42525586993419\n",
      "Root Mean Square Error for Stochastic Gradient Descent: 173.13159885067267\n"
     ]
    }
   ],
   "source": [
    "# Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm\n",
    "def calcRMSE(X, Y, params):\n",
    "    Y_cap = np.zeros(len(Y))\n",
    "    for i in range(0, len(Y)):\n",
    "        Y_cap[i] = np.dot(params, [1.0, X[i]])\n",
    "    E = Y - Y_cap\n",
    "    return np.sqrt(np.sum(E*E)/len(E))\n",
    "\n",
    "print(\"Root Mean Square Error for Batch Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat_batch)))\n",
    "print(\"Root Mean Square Error for Stochastic Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1630902274461,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "930loAL6kq_L",
    "outputId": "e807576e-9852-4857-9a29-d367f2e0b26b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min cost with BGD: 14340.238833219535\n",
      "min cost with SGD: 38766.73478538944\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
    "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('normalized cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
    "print(f'min cost with SGD: {np.min(cost)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrpju6Kwkq_N"
   },
   "source": [
    "### Q2.4 Which linear regression model do you think works best for this data? Explain in brief. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbTux39kq_N"
   },
   "source": [
    "Based on how data is created, we have:\n",
    "\\begin{equation}\n",
    "y = 15 x + 2.4 + 300.0 * uniform(0, 1)\n",
    "\\end{equation}\n",
    "Thus, the model that works best for this data would be the one that provides the average results.\n",
    "The expected value of $uniform(0, 1)$ is $0.5$. Thus we have:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y &= 15 x + 2.4 + 300.0 * E(uniform(0, 1))\\\\\n",
    "y &= 15 x + 2.4 + 300 * (0.5)\\\\\n",
    "y &= 152.4 + 15x\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This will fit best with data and hence the best linear regression model.\n",
    "Comparing it with $y = \\theta_0 + \\theta_1 x$, we have $\\theta_0 = 152.4$ and $\\theta_1 = 15$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p02LYZrkq_N"
   },
   "source": [
    "### Q3. Linear Regression Analytical Problem (10 points)\n",
    "Consider the following training data.\n",
    "\n",
    "| X1 | X2 | Y |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1.5 |\n",
    "| 1 | 0 | 2 |\n",
    "| 1 | 1 | 2.5 |\n",
    "Suppose the data comes from a model y = $_{0}$ +$_{1}$x1 +$_{2}$x2 for unknown constants $_{0}$,$_{1}$,$_{2}$. Use least squares linear regression to find an estimate of $_{0}$,$_{1}$,$_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have:\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "        1 & 0 & 0\\\\\n",
    "        1 & 0 & 1\\\\\n",
    "        1 & 1 & 0\\\\\n",
    "        1 & 1 & 1\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Y = \\begin{bmatrix}\n",
    "        0\\\\\n",
    "        1.5\\\\\n",
    "        2\\\\\n",
    "        2.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "X^T = \\begin{bmatrix}\n",
    "        1 & 1 & 1 & 1\\\\\n",
    "        0 & 0 & 1 & 1\\\\\n",
    "        0 & 1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence,\n",
    "\\begin{equation}\n",
    "X^TX = \\begin{bmatrix}\n",
    "        4 & 2 & 2\\\\\n",
    "        2 & 2 & 1\\\\\n",
    "        2 & 1 & 2\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "|X^TX| = 4(4-1) -2(4-2)+2(2-4)=4\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Cf(X^TX) = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Adj(X^TX) = (Cf(X^TX))^T = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{|X^TX|} Adj(X^TX)= \\begin{bmatrix}\n",
    "        0.75 & -0.5 & -0.5\\\\\n",
    "        -0.5 & 1 & 0\\\\\n",
    "        -0.5 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1}X^T = \\begin{bmatrix}\n",
    "        0.75 & 0.25 & 0.25 & -0.25\\\\\n",
    "        -0.5 & -0.5 & 0.5 & 0.5\\\\\n",
    "        -0.5 & 0.5 & -0.5 & 0.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We have, $\\theta^* = (X^TX)^{-1}X^TY$\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "\\theta^* = \\begin{bmatrix}\n",
    "        \\theta_0\\\\\n",
    "        \\theta_1\\\\\n",
    "        \\theta_2\n",
    "    \\end{bmatrix}\n",
    "    =\\begin{bmatrix}\n",
    "        0.25\\\\\n",
    "        1.5\\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the linear regression equation becomes:\n",
    "\\begin{equation}\n",
    "y = 0.25 + 1.5X_1 + X_2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-1-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}