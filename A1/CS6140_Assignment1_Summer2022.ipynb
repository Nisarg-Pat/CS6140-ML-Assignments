{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1avDQgkq-5"
   },
   "source": [
    "# CS 6140 Machine Learning: Assignment - 1 (Total Points: 100)\n",
    "## Prof. Ahmad Uzair "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFpvDelkq-6"
   },
   "source": [
    "### Q1. Decision Tree Classifier (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Growing Decison Trees from scratch (40 points)\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "You must also print the Decision Tree. Use information gain based on entropy as the splitting measure.\n",
    "\n",
    "Use the data.csv dataset for this particular question. The dataset should be uploaded on Canvas with Assignment 1. Split the dataset into training and test data and calculate testing accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utils class to perform certain calculations\n",
    "class Utils:\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)\n",
    "\n",
    "# Node of a DecisionTree. Can be either regular node or leaf node.\n",
    "# Normal node contains information about the feature and the value it compares in that node.\n",
    "# Leaf node contains the type of class.\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, X, Y):\n",
    "        if len(X) == 0:\n",
    "            return\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.isLeaf = False\n",
    "        self.classType = -1\n",
    "        self.H = self.entropy(Y)\n",
    "        self.trueChild = None\n",
    "        self.falseChild = None\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        for i in range(len(freq)):\n",
    "            if freq[i] == len(Y):\n",
    "                self.isLeaf = True\n",
    "                self.classType = num[i]\n",
    "                return\n",
    "\n",
    "        self.featureIndex, self.compValue = self.findBestSplit()\n",
    "        tx, ty, fx, fy = self.split(X, Y, self.featureIndex, self.compValue)\n",
    "        self.trueChild = DecisionTreeNode(tx, ty)\n",
    "        self.falseChild = DecisionTreeNode(fx, fy)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        h = 0.0\n",
    "        for val in freq:\n",
    "            if val != 0:\n",
    "                prob = val / len(Y)\n",
    "                h -= prob * (np.log2(prob))\n",
    "        return h\n",
    "\n",
    "    def informationGain(self, X, Y, featureIndex, value):\n",
    "        tx, ty, fx, fy = self.split(X, Y, featureIndex, value)\n",
    "        expectedEntropy = 0\n",
    "        expectedEntropy += (len(ty) / len(Y)) * self.entropy(ty)\n",
    "        expectedEntropy += (len(fy) / len(Y)) * self.entropy(fy)\n",
    "        IG = self.H - expectedEntropy\n",
    "        return IG\n",
    "\n",
    "    def split(self, X, Y, featureIndex, value):\n",
    "        tx, ty, fx, fy = [], [], [], []\n",
    "        for i in range(0, len(X)):\n",
    "            if X[i][featureIndex] < value:\n",
    "                tx.append(X[i])\n",
    "                ty.append(Y[i])\n",
    "            else:\n",
    "                fx.append(X[i])\n",
    "                fy.append(Y[i])\n",
    "        return np.array(tx), np.array(ty), np.array(fx), np.array(fy)\n",
    "\n",
    "    def findBestSplit(self):\n",
    "        copy_X = np.transpose(self.X)\n",
    "        maxIG = float(\"-inf\")\n",
    "        bestFeatureIndex = None\n",
    "        bestValue = None\n",
    "        for i in range(0, len(copy_X)):\n",
    "            T = np.sort(copy_X[i])\n",
    "            for j in range(1, len(T)):\n",
    "                midValue = (T[j - 1] + T[j]) / 2.0\n",
    "                currentIG = self.informationGain(self.X, self.Y, i, midValue)\n",
    "                if currentIG > maxIG:\n",
    "                    maxIG = currentIG\n",
    "                    bestFeatureIndex = i\n",
    "                    bestValue = midValue\n",
    "        return bestFeatureIndex, bestValue\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.isLeaf:\n",
    "            return self.classType\n",
    "        elif X[self.featureIndex] <= self.compValue:\n",
    "            return self.trueChild.predict(X)\n",
    "        else:\n",
    "            return self.falseChild.predict(X)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.isLeaf:\n",
    "            return \"class:\" + str(self.classType)\n",
    "        else:\n",
    "            return \"feature\" + str(self.featureIndex + 1) + \" <= \" + str(self.compValue)\n",
    "\n",
    "#The Decision Tree Class\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.root = DecisionTreeNode(X, Y)\n",
    "        return\n",
    "\n",
    "    def print(self, feature_names, class_names):\n",
    "        self.preOrder(self.root, feature_names, class_names, \"|--- \")\n",
    "\n",
    "    def preOrder(self, root, feature_names, class_names, prev):\n",
    "        if root == None:\n",
    "            return\n",
    "        if root.isLeaf:\n",
    "            print(prev + \"class: \" + class_names[root.classType])\n",
    "            return\n",
    "        print(prev + feature_names[root.featureIndex] + \" <= \" + str(root.compValue))\n",
    "        self.preOrder(root.trueChild, feature_names, class_names, \"|   \" + prev)\n",
    "        print(prev + feature_names[root.featureIndex] + \" >  \" + str(root.compValue))\n",
    "        self.preOrder(root.falseChild, feature_names, class_names, \"|   \" + prev)\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y = []\n",
    "        for i in range(len(X)):\n",
    "            Y.append(self.root.predict(X[i]))\n",
    "        return np.array(Y)\n",
    "\n",
    "    def accuracy(self, Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature4 <= 1.65\n",
      "|   |   |--- feature3 <= 5.0\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  5.0\n",
      "|   |   |   |--- feature1 <= 6.05\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  6.05\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.65\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature2 <= 3.1\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature2 >  3.1\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "Testing Accuracy: 94.74\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_csv(\"data.csv\")\n",
    "X = data.drop(\"class\", axis=1).values\n",
    "Y = data[\"class\"].values\n",
    "\n",
    "feature_names = list(data.drop(\"class\", axis=1).columns)\n",
    "class_names = [str(i) for i in range(0, len(set(Y)))]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "dt.print(feature_names, class_names)\n",
    "\n",
    "Y_test_pred = dt.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Decision Tree using Sklearn Library (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Decision Tree Classifier from the Sklearn Library and use gini index as a splitting measure. Use the data.csv dataset.\n",
    "Calculate accuracy for this model. \n",
    "Print the Decision tree and compare the Decision Trees generated from your code and Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature4 <= 1.65\n",
      "|   |   |--- feature3 <= 5.00\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  5.00\n",
      "|   |   |   |--- feature1 <= 6.05\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  6.05\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.65\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature2 <= 3.10\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature2 >  3.10\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "\n",
      "Testing Accuracy: 94.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(export_text(clf, feature_names=feature_names))\n",
    "\n",
    "Y_test_pred = clf.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Comparision of the two generated decision trees:\n",
    "\n",
    "Both the trees generated are almost identical most of the time.\n",
    "The difference in the trees could occur because of the different splitting measure (Information Gain for my code and Gini for sklearn code). The difference is still minimal and the accuracy achieved by both of the trees is similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEBH56pukq_H"
   },
   "source": [
    "### Q2 Linear Regression (40 points)\n",
    "\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FL1tuQEkq_H"
   },
   "source": [
    "## Gradient descent algorithm \n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This minimizes the following cost function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "h(x_i) = \\theta^T \\bar{x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cOem1EjQkq_H"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "true_slope = 15\n",
    "true_intercept = 2.4\n",
    "input_var = np.arange(0.0,100.0)\n",
    "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1630902228487,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "SNvDqYEykq_H",
    "outputId": "c7f53823-73d9-473f-9e28-944f1f09a415",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAiuklEQVR4nO3df7BcZZ3n8feHEOSCM1zADAs3yRDHGAu1JMwdYCa7sxgdAsialOUAO7NLZNlN1aw6gooGd2rwx8wYxRrE0mI2K4xQ60IQWciOllnGaLHFCnJjovyS4haIyTWYOCTM7CRiAt/9o0+TTqdPn+7b50f/+LyqUrf79Onup6vhfPt5vs/zfRQRmJmZtXNU1Q0wM7P+52BhZmaZHCzMzCyTg4WZmWVysDAzs0wOFmZmlqmwYCHpFkm7JD3acOxMSQ9K2iZpStLZyXFJ+oKkaUk/knRWw3NWS3oq+be6qPaamVm6InsWXwEuaDr2WeATEXEm8OfJfYALgcXJvzXATQCSTgKuA84Bzgauk3RigW02M7MWCgsWEXE/8HzzYeDXk9snAD9Lbq8EbouaB4FxSacCK4D7IuL5iNgD3MeRAcjMzAp2dMnvdxWwSdLnqAWq30uOTwDbG87bkRxLO34ESWuo9Uo4/vjjf/sNb3hDrg03Mxt2W7Zs+UVEzGv1WNnB4k+AqyPi65IuAW4G3p7HC0fEemA9wOTkZExNTeXxsmZmI0PSs2mPlT0bajVwd3L7a9TyEAAzwIKG8+Ynx9KOm5lZicoOFj8D/nVyeznwVHJ7I3B5MivqXOCFiNgJbALOl3Riktg+PzlmZmYlKmwYStLtwHnAayTtoDar6T8BN0o6GvglSY4B+CZwETAN7AOuAIiI5yV9Cng4Oe+TEdGcNDczs4JpGEuUO2dhZtY9SVsiYrLVY17BbWZmmcqeDWVmZjm5Z+sM1296kp/t3c9p42Ncs2IJq5a2XF3QMwcLM7MBdM/WGa69+xH2H3gJgJm9+7n27kcACgkYHoYyMxtA12968pVAUbf/wEtcv+nJQt7PwcLMbAD9bO/+ro73ysHCzGwAnTY+1tXxXjlYmJkNoGtWLGFs7pzDjo3NncM1K5YU8n5OcJuZDaB6EtuzoczMrK1VSycKCw7NPAxlZmaZHCzMzCyTg4WZmWVysDAzs0wOFmZmlsmzoczM+lCZRQI74WBhZtZnyi4S2InChqEk3SJpl6RHm46/X9KPJT0m6bMNx6+VNC3pSUkrGo5fkByblrS2qPaamfWLsosEdqLInsVXgC8Ct9UPSHorsBJ4S0S8KOk3kuNnAJcBbwROA/5e0uuTp30J+ANgB/CwpI0R8XiB7TYzq1TZRQI7UViwiIj7JZ3edPhPgHUR8WJyzq7k+ErgjuT4M5KmgbOTx6Yj4mkASXck5zpYmNnAyspHnDY+xkyLwFBUkcBOlJ2zeD3wryT9JfBL4MMR8TAwATzYcN6O5BjA9qbj57R6YUlrgDUACxcuzLnZZmb5aJePgNoQ1Mze/QiIhucVWSSwE2UHi6OBk4Bzgd8B7pT02jxeOCLWA+sBJicnI+N0M7NKpOUjPr7xMV48+PIrjwW8EjAmRnA21A7g7ogI4PuSXgZeA8wACxrOm58co81xM7OBk5Z32Lv/wBHH6oHigbXLC25VtrIX5d0DvBUgSWAfA/wC2AhcJulVkhYBi4HvAw8DiyUtknQMtST4xpLbbGaWm27zDlUmtRsV1rOQdDtwHvAaSTuA64BbgFuS6bS/AlYnvYzHJN1JLXF9EHhvRLyUvM77gE3AHOCWiHisqDabmXWr28Vz16xYcljOAmr5iGPnHsWefUf2LqpMajdS7Vo9XCYnJ2NqaqrqZpjZkKoHiLRE9Kff9ea2AaNVgAFaBpGs18qTpC0RMdnqMa/gNjNL0clFvfnndn3xXLsLfLtNi/qpxEcjBwszsxbSprgeO/eoI2YzNZttnqHMne+65WBhZtZC2hTXrEABxeYZqiow6GBhZtbCbHsH9cVzeV7U03IkrRb0FRVEHCzMzFpIK7kxPjb3sMVzcOTiOSC3qrHNw2GtciTNC/qKqFLr2VBmZi00X6Th0OwkaP8rftm6zS0DDXS/Grvda2XpdkGfZ0OZmXWpfjFPCwrtLvbthrC6/dXfy6K8PBf0OViYmaWY7eyktCGsuk6m13b6WmUt6PMe3GZmHbhn6wzL1m1m0dpvsGzdZu7Zml6m7poVSxibO6ft69V/9We9bqvXUvJ3YnyMT7/rzVz3b954xDl5V6l1z8LMLEO325w2DmGl9QpOGx/r6HWzhsMaFTkbygluM7MMaUnmThLI7RLlacGkqkqzTnCb2VCoakFaL9uctusZXL1h26xft2wOFmY2ELodCspTr9ucpiXK+3H71DROcJvZQEgrv3H9picLf+9WSeY8EshFvW4R3LMws4HQy1BQr7pJMvfD6xahyM2PbgEuBnZFxJuaHvsQ8DlgXkT8QpKAG4GLgH3AeyLiB8m5q4E/S576FxFxa1FtNrP+VfWQTVEVYfu50myjIoehvgJc0HxQ0gLgfOCnDYcvpLaV6mJgDXBTcu5J1HbYOwc4G7hO0okFttnM+tQgDdkMo8J6FhFxv6TTWzx0A/AR4N6GYyuB25ItVh+UNC7pVGrbst4XEc8DSLqPWgC6vah2m1n12s16KnrIpqoZV/2u1JyFpJXATET8sDby9IoJYHvD/R3JsbTjrV57DbVeCQsXLsyx1WZWpqxZT0VeuKuccdXvSpsNJek44GPAnxfx+hGxPiImI2Jy3rx5RbyFmZWgyllPVb53vytz6uxvAYuAH0r6CTAf+IGkfwHMAAsazp2fHEs7bmZDqspZT1W+d78rLVhExCMR8RsRcXpEnE5tSOmsiHgO2AhcrppzgRciYiewCThf0olJYvv85JiZDam02U1lzHqq8r37XWHBQtLtwPeAJZJ2SLqyzenfBJ4GpoH/BvxngCSx/Sng4eTfJ+vJbjMbfK0qrlY566nX9+6mMu2gcSFBM6tELzvRFd2u2bx3u88zKMnxdoUEHSzMrBK9VHLtR8PweVx11sz6Rv2Xe9o+D4OaTB725LgLCZpZaepDNe22CR3UZPKwJ8cdLMysNK3WMTQa5PIdw16OxMNQZlaadkMyEyUksoss5TFIFWRnw8HCzHKTdTFOqxxbRhK4jFIeg1JBdjYcLMwsF51cjK9ZsaTl9NIyhmqySnkMa48gL85ZmFkuOqmrtGrpBJ9+15uZGB9D1HoUZa1DSBsCqwe1mb37iYb7w7SgLg/uWZhZLjqdOlrVUE3aENgcKTXIuXdxiHsWZtaTeomLtOW9/TJ1tNVsJQEvpSxMHpb1EXlxz8LMZq1ViYtG/TR1tHG20sze/QhSAxz0T5DrFw4WZtZSJ9NM262bKGMqbCdafY52K8ihv4Jcv3CwMLMjdDrNNG2oRpDbVNhe1kakfY52CwP7Jcj1GwcLMztCu5lNq5ZOvHIBLzpPMdu1Ee3qT+0/8BJzpJa5ikEq+lc2J7jN7AjtZjZl1XfKcwhnNtucdlJ/6qWIoS7NUYQiNz+6RdIuSY82HLte0o8l/UjS/5Q03vDYtZKmJT0paUXD8QuSY9OS1hbVXjM7pF1RvKw8RZ7rJjqdjtu46dCH7vxh22GmxnZWsd5jUBU5DPUV4IvAbQ3H7gOujYiDkj4DXAt8VNIZwGXAG4HTgL+X9PrkOV8C/oDaNqwPS9oYEY8X2G6zkddupfXVG7a1fE6eeYq6tLURjcGseagqbSpsXf1zDHNpjiIUFiwi4n5Jpzcd+98Ndx8E3p3cXgncEREvAs9ImgbOTh6bjoinASTdkZzrYGHWgW6Tw43nnzA2l2PnHsXefQcOe25aLqCIqabtglbWvhitOHk9e1UmuP8DsCG5PUEteNTtSI4BbG86fk6rF5O0BlgDsHDhwlwbajaIuk0ON5+/d/8BxubO4YZLzzzs/DLrO6VVcgUyZzU1GrTtTftRJcFC0n8BDgJfzes1I2I9sB5q26rm9bpmgyotOXzVhm1cv+nJI35hZ82Aqiu7FHer4aJl6zZnBoo5Ei9HuDBgTkoPFpLeA1wMvC0ObQA+AyxoOG1+cow2x82sjXblKlr1MrrZFrTq8f6sUhzuSeSv1Kmzki4APgK8MyL2NTy0EbhM0qskLQIWA98HHgYWS1ok6RhqSfCNZbbZbBA0zgZatm4z92ydycwhNE9BHaRtQdu1yTObilHk1Nnbge8BSyTtkHQltdlRvwbcJ2mbpL8BiIjHgDupJa6/Bbw3Il6KiIPA+4BNwBPAncm5ZpZoXFfQWGL7rW+Yd8RagmaNv9AHaVvQtLZ+/tIzeWDtcgeKAigyppkNosnJyZiamqq6GWaFypoNNNFBHaTmFctFbjuat0Fq66CQtCUiJls+5mBhNniyqr1Cbd3DM+vekXq+x/WtWbtg4XIfZgOo3SrqusZx/eYd6saTNRRXb9j2So7DrB0HC7MB1MlsoOZcw6qlEzywdjk3XHomLx58mT37DngbUeuYg4XZAOplNtBsivOVodWMLusfLlFuNoDSVlF3koPoZj1FWWZbitzK456F2QBqzkF0s7agH9dT9Gtvxw5xz8JsQM12FXWZtZ061Y+9HTucg4XZiJltbadeKthmnd9JKXKrloOF2QjqtlfSawXbrPP7sbdjh3POwswydZtTaFfxttVMp15yMFYO9yzMLFO3OYVuK97Wbzs49C/3LMwsU7czqLqteGv9z8HCzDJ1W5G21fnNPNNpsHgYyswydTuDqvH8tIq3nuk0WFx11mzEFV3q2xVvB0clVWcl3SJpl6RHG46dJOk+SU8lf09MjkvSFyRNS/qRpLManrM6Of8pSauLaq/ZKErbOCnPukye6TQcCutZSPp94P8Bt0XEm5JjnwWej4h1ktYCJ0bERyVdBLwfuAg4B7gxIs6RdBIwBUwCAWwBfjsi9rR7b/cszDqzbN3mlsNEzZsi2WiopGcREfcDzzcdXgncmty+FVjVcPy2qHkQGJd0KrACuC8ink8CxH3ABUW12WzUuMyGdarsBPcpEbEzuf0ccEpyewLY3nDejuRY2nGzgdcP24K6zIZ1qrLZUBERknIbA5O0BlgDsHDhwrxe1qxnrYIC0BcluV1mwzpV9jqLnyfDSyR/dyXHZ4AFDefNT46lHT9CRKyPiMmImJw3b17uDTebjbQE8if+12N9UZLbyWfrVNk9i43AamBd8vfehuPvk3QHtQT3CxGxU9Im4K/qs6aA84FrS26z2ayl1UhK2z+7ilyBy2xYJwoLFpJuB84DXiNpB3AdtSBxp6QrgWeBS5LTv0ltJtQ0sA+4AiAinpf0KeDh5LxPRkRz0tysEHmU5O724u9cgfUrL8oza6HbhWRp5x879yj27DtwxPnjY3N58eDLXqhmfaWSqbNmgyyvktwRtKyp9PF3vtG5Ahsorg1l1kJeJblf2H+AGy49M3U4q4jg0A9Tcm34OFiYtZC2/iCorXpuvgC3W69QZgK52x3qzDrlYSizFtqV2G5VP6nbEt5F6Xb4zKxTDhZmLTSuP2il+QLcL+sVXL7DipI5DCXp/cB/zyreZzZs6sNHi9Z+g1ZzBpsvwP2wXsHlO6wonfQsTgEelnSnpAskqehGmRXtnq0zLFu3mUVrv8GydZvbluSe7dahVeiX4TAbPpnBIiL+DFgM3Ay8B3hK0l9J+q2C22ZWiG73cBikC3C/DIfZ8Ol4UZ6kt1BbWX0B8B3gXGrlwz9SXPNmx4vyrJ3Z7OHQOB31hLG5SLB334Gup6a2m9aa9pinwlpZ2i3K6yRn8QHgcuAXwJeBayLigKSjgKeAvgsWZpB+8Z1NEriej+hlamq750LrKrRTzz7P17fMeCqsVa6TdRYnAe+KiGcbD0bEy5IuLqZZZr1pd2HuJQncbmpq1sU7a1prq8duf2g7LzX1/jt9P7M8dZKzuK45UDQ89kT+TTLrXbsLc7scRFbiu5epqe2em/ZYc6Do5v3M8uR1FjaU2l2Y05LAQGbiu5eZUe2e2+3Mqn6ciWXDzcHChlLWRX3V0gkeWLucZ9a9gwfWLmfV0omOVj+36pWIWmDJmoLbrkfTbsV4s07fzyxPrg1lAylrhtBstgvtZIip/h7Xb3qSmb37EbyyYC8r+dz43LR21183TTfvZ5Yn72dhA6fTvSa6nXLa7ZTa2UzB7UTa686RWuYwen0/s7q+289C0tWSHpP0qKTbJR0raZGkhyRNS9og6Zjk3Fcl96eTx0+vos3WPzotltdqqKmdbhffFVWHKa0dTnZblUoPFpImgD8FJiPiTcAc4DLgM8ANEfE6YA9wZfKUK4E9yfEbkvNshBV1ke529XNRZUDS2pFW1NDJbitDVTmLo4ExSQeA44CdwHLgj5LHbwU+DtwErExuA9wFfFGSYhjHz6wjRRbL66YY4GzyIr22o6j3M8tSes8iImaAzwE/pRYkXgC2AHsj4mBy2g6g/n/KBLA9ee7B5PyTm19X0hpJU5Kmdu/eXeyHsEr1S62msuswue6TVan0noWkE6n1FhYBe4GvUas31ZOIWA+sh1qCu9fXs/7VyayiMtvS6n2LqufUD2XQbTRVMQz1duCZiNgNIOluYBkwLunopPcwH6hPIJ8BFgA7JB0NnAD8Q/nNtn7SeNGsX5iv3rCtLwrteWtTG0ZVzIb6KXCupOOSvTHeBjxOrZLtu5NzVgP3Jrc3JvdJHt/sfIXVdVtuvAze2tSGURU5i4eoJap/ADyStGE98FHgg5KmqeUkbk6ecjNwcnL8g8Dastts/asfL8ze2tSGUSWzoSLiOuC6psNPA2e3OPeXwB+W0S4bPP14YfbWpjaMXBvKBlo/bnnaL7O1zPLkYGEDrR8vzJ7iasPIhQRtoPXTNNrmdlXdBrM8OVjYwPOF2ax4DhY2MopaKGc2ChwsbCR4oZxZbxwsrDJl/tJvtx7DwcIsm4OFVaLsX/rt1mN4eMosm6fOWiXKXnmdtu7ihLG5fVcuxKwfOVhYqe7ZOpO6bSgUt/I6bT2GRN+VCzHrRw4WVprGon9pilp5nbZQbu++Ay3Pdx0ns8M5Z2GlaTX01EjUhoGWrdtcSN6g1XqM6zc96TpOZh1wz8JK0+7XuoB63fky8wb9WC7ErB85WFhp0n6tz5Fo3qCkrLyB6ziZdcbDUFaaa1YsOWy6LNR+xacNTZWVN3C5ELNslfQsJI1LukvSjyU9Iel3JZ0k6T5JTyV/T0zOlaQvSJqW9CNJZ1XR5lFWn8G0aO03WLZu86yHh9J+xU/0YZlxMztcVT2LG4FvRcS7JR0DHAd8DPh2RKyTtJbajngfBS4EFif/zgFuSv5aCfJePJf2K75Vj+OaFUu6XjDnBXZmxVDZ21lLOgHYBry2cS9tSU8C50XETkmnAt+NiCWS/mty+/bm89LeY3JyMqampgr9HKMibU3ExPgYD6xdntv7NF7kTxibiwR79h04LPENtSCSllNoDmxZ55vZ4SRtiYjJVo9VMQy1CNgN/K2krZK+LOl44JSGAPAccEpyewLY3vD8Hcmxw0haI2lK0tTu3bsLbP5oKWvb0lVLJ3hg7XJuuPRMXjz4MnuS9Q/dJL77cT9us2FRRbA4GjgLuCkilgL/TG3I6RVJj6OrLk9ErI+IyYiYnDdvXm6NHXVlb1uatRYDDq3FaM6d9ON+3GbDoopgsQPYEREPJffvohY8fp4MP5H83ZU8PgMsaHj+/OSYlaDsdQidXthbrcXox/24zYZF6cEiIp4DtkuqX23eBjwObARWJ8dWA/cmtzcClyezos4FXmiXr7B8lb0OoZsLe/MQkxfYmRWnqtlQ7we+msyEehq4glrgulPSlcCzwCXJud8ELgKmgX3JudalXmYJlbkOodVajOYkd6PGnki/7sdtNgwqCRYRsQ1olXF/W4tzA3hv0W0aZnlOfy16amraBb/TGk5eYGdWDK/gHgF57RJX1oZF3a7FMLPiuTbUCMhrllCVU1Ndw8msWu5ZjIDTxsdyKcNd9dRUDzGZVcc9ixGQ1ywhT001G10OFiMgryEcT001G10ehhoyabOV8hjC8dRUs9HlYDFEep2t1Mm0WOcNzEaTg8UQ6WWKbJHTYl023GzwOVgMkV5mK+W1FqOuHiBm9u5vub825Ls2w8yK5QT3AMnasa6X2Up5Tout91Lq03Wr2l/bzPLjYDEgGi/AQeuqq73MVspjWmw9mF21YVtmmXGXDTcbLA4WA6KT1dO9TJHtdVpsc28ii9dmmA0W5ywGRKfDRLOdrdTrtNhONi2q89oMs8HjYDEg8irZ0U4v02KzhpXqSe4Jz4YyG0gOFgOi1T4PefxCz2taa1owAwcIs2FQWc5C0hxJWyX9XXJ/kaSHJE1L2pBsjISkVyX3p5PHT6+qzVUqoupqJ0nzTqXlPD5/6Zk8sHa5A4XZgKuyZ/EB4Ang15P7nwFuiIg7JP0NcCVwU/J3T0S8TtJlyXmXVtHgquW9ejorad5Jj6OxZ3LC2FyOnXsUe/cd8OI7syFTSbCQNB94B/CXwAclCVgO/FFyyq3Ax6kFi5XJbYC7gC9KUrKD3lCoaoVzWp6h3sPIWs3dvOp77/4DjM2dww2XnukgYTZkqhqG+jzwEeDl5P7JwN6IOJjc3wHUrzYTwHaA5PEXkvOHQp5DQd1KS47PkTra5KjKzZDMrFylBwtJFwO7ImJLzq+7RtKUpKndu3fn+dJdy1pp3ajKC25anuGllE5bc0+k6s2QzKw8VfQslgHvlPQT4A5qw083AuOS6sNi84H6FXYGWACQPH4C8A/NLxoR6yNiMiIm582bV+wnaKPbnkKRF9ysoJWWNJ/ocDW3N0MyGx2l5ywi4lrgWgBJ5wEfjog/lvQ14N3UAshq4N7kKRuT+99LHt/cz/mKbgvyFbV+otMqsmlJ806m6RY1ndfM+k8/lfv4KLVk9zS1nMTNyfGbgZOT4x8E1lbUvo5021Moave5Xoa3Op2mW8R0XjPrT5UuyouI7wLfTW4/DZzd4pxfAn9YasN60G1PIavMxmxnSvU6vNXpNF1vhmQ2GryCO2ezGZpJu+D2siFRGeVBzGx09NMw1FDIc2iml6Gkooa3zGw0uWfRgW6HgvIamullKKnXKrJmZo0cLDIUuTd1ll6HkpxPMLO8eBgqQz8umsuj0myniwbNzMA9i0xVrlIuYiipyp6SmQ0uB4sMVc8qKrPSrIOFmaXxMFSGPPam7qchH9dzMrPZcM8iQy9DQe2GfGb7mr2quqdkZoNJfVxmadYmJydjamqq6mawbN3m1K1G63tS143NnVNKqYzmAFbme5tZf5O0JSImWz3mnkWB2g3tNIfobvMGsy0D4vUXZjYbDhYFShvySdNp3qDXGU1ef2Fm3XKCu0CtkuPtdJo3SJvRdNWGbX2RRDez4eOeRYEah3yyehjdzLBq1wPxugkzK4J7FgVbtXSCB9Yu5/OXnnlEL0PJ326LDWb1QLwPtpnlzT2LkuSZWG5VBr2Z102YWZ5KDxaSFgC3AadQmxS0PiJulHQSsAE4HfgJcElE7JEkant0XwTsA94TET8ou91ZOpmdlFdiuZPhLa+bMLM8VTEMdRD4UEScAZwLvFfSGdS2S/12RCwGvs2h7VMvBBYn/9YAN5Xf5Pbqs5Nm9u4nOJQ3mE2iudMV3+2Gt7xvhZnlrfRgERE76z2DiPgn4AlgAlgJ3JqcdiuwKrm9Ergtah4ExiWdWm6r28urMu1sgo73wTazMlSas5B0OrAUeAg4JSJ2Jg89R22YCmqBZHvD03Ykx3Y2HEPSGmo9DxYuXFhco1vIq97SbIv8ed2EmRWtstlQkl4NfB24KiL+sfGxqNUg6aoOSUSsj4jJiJicN29eji3NlpYf6DZv4CJ/ZtavKgkWkuZSCxRfjYi7k8M/rw8vJX93JcdngAUNT5+fHOsbeW1SlFfQMTPLW+nBIpnddDPwRET8dcNDG4HVye3VwL0Nxy9XzbnACw3DVaVrlYDOK29Q1M54Zma9qiJnsQz498AjkrYlxz4GrAPulHQl8CxwSfLYN6lNm52mNnX2ilJb2yCrJlOveQMX+TOzfuUS5V1IKzk+MT7GA2uX5/5+ZmZlconyDjUurDthbC4S7N134JVf+E5Am9mocrBINA8x7d1/4JXH6sNN48fNZc++A0c81wloMxt2LiSYaLXGodH+Ay8RgRPQZjaSHCwSnQwlvbD/gFdLm9lI8jBUopNd7U4bHzts1lM9x3H1hm2euWRmQ809i0TWrnbNw015Fg80M+t3DhaJ5oV142NzOfG4uanDTXkVDzQzGwQehmrQzcI6T6M1s1HinsUsuY6TmY0SB4tZch0nMxslHoaaJddxMrNR4mDRA286ZGajwsNQZmaWycHCzMwyOViYmVkmBwszM8vkYGFmZpmGcqc8Sbupbc06W68BfpFTcwbFKH5mGM3PPYqfGUbzc3f7mX8zIua1emAog0WvJE2lbS04rEbxM8Nofu5R/Mwwmp87z8/sYSgzM8vkYGFmZpkcLFpbX3UDKjCKnxlG83OP4meG0fzcuX1m5yzMzCyTexZmZpbJwcLMzDI5WDSQdIGkJyVNS1pbdXuKImmBpO9IelzSY5I+kBw/SdJ9kp5K/p5YdVvzJmmOpK2S/i65v0jSQ8l3vkHSMVW3MW+SxiXdJenHkp6Q9LvD/l1Lujr5b/tRSbdLOnYYv2tJt0jaJenRhmMtv1vVfCH5/D+SdFY37+VgkZA0B/gScCFwBvBvJZ1RbasKcxD4UEScAZwLvDf5rGuBb0fEYuDbyf1h8wHgiYb7nwFuiIjXAXuAKytpVbFuBL4VEW8A3kLt8w/tdy1pAvhTYDIi3gTMAS5jOL/rrwAXNB1L+24vBBYn/9YAN3XzRg4Wh5wNTEfE0xHxK+AOYGXFbSpEROyMiB8kt/+J2sVjgtrnvTU57VZgVSUNLIik+cA7gC8n9wUsB+5KThnGz3wC8PvAzQAR8auI2MuQf9fU9uoZk3Q0cBywkyH8riPifuD5psNp3+1K4LaoeRAYl3Rqp+/lYHHIBLC94f6O5NhQk3Q6sBR4CDglInYmDz0HnFJVuwryeeAjwMvJ/ZOBvRFxMLk/jN/5ImA38LfJ8NuXJR3PEH/XETEDfA74KbUg8QKwheH/ruvSvtuernEOFiNM0quBrwNXRcQ/Nj4WtTnVQzOvWtLFwK6I2FJ1W0p2NHAWcFNELAX+maYhpyH8rk+k9it6EXAacDxHDtWMhDy/WweLQ2aABQ335yfHhpKkudQCxVcj4u7k8M/r3dLk766q2leAZcA7Jf2E2hDjcmpj+ePJUAUM53e+A9gREQ8l9++iFjyG+bt+O/BMROyOiAPA3dS+/2H/ruvSvtuernEOFoc8DCxOZkwcQy0htrHiNhUiGau/GXgiIv664aGNwOrk9mrg3rLbVpSIuDYi5kfE6dS+280R8cfAd4B3J6cN1WcGiIjngO2SliSH3gY8zhB/19SGn86VdFzy33r9Mw/1d90g7bvdCFyezIo6F3ihYbgqk1dwN5B0EbVx7TnALRHxl9W2qBiS/iXwf4BHODR+/zFqeYs7gYXUSrxfEhHNybOBJ+k84MMRcbGk11LraZwEbAX+XUS8WGHzcifpTGpJ/WOAp4ErqP1QHNrvWtIngEupzfzbCvxHauPzQ/VdS7odOI9aKfKfA9cB99Diu00C5xepDcntA66IiKmO38vBwszMsngYyszMMjlYmJlZJgcLMzPL5GBhZmaZHCzMzCyTg4WZmWVysDAzs0wOFmYlkPQ7yR4Cx0o6Ptlr4U1Vt8usU16UZ1YSSX8BHAuMUavX9OmKm2TWMQcLs5IkNcceBn4J/F5EvFRxk8w65mEos/KcDLwa+DVqPQyzgeGehVlJJG2kVshuEXBqRLyv4iaZdezo7FPMrFeSLgcORMT/SPZ7/7+SlkfE5qrbZtYJ9yzMzCyTcxZmZpbJwcLMzDI5WJiZWSYHCzMzy+RgYWZmmRwszMwsk4OFmZll+v9EejmhPCyX1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.scatter(input_var, output_var)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XgNhbpEmkq_I"
   },
   "outputs": [],
   "source": [
    "def compute_cost(ip, op, params):\n",
    "    \"\"\"\n",
    "    Cost function in linear regression where the cost is calculated\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    num_samples = len(ip)\n",
    "    cost_sum = 0.0\n",
    "    for x,y in zip(ip, op):\n",
    "        y_hat = np.dot(params, np.array([1.0, x]))\n",
    "        cost_sum += (y_hat - y) ** 2\n",
    "    \n",
    "    cost = cost_sum / (num_samples)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Implement Linear Regression using Batch Gradient Descent from scratch.  (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ao7aYu9kq_I"
   },
   "source": [
    "\n",
    "### Batch gradient descent\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f0z876gDkq_I"
   },
   "outputs": [],
   "source": [
    "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(max_iter)\n",
    "    params_store = np.zeros([2, max_iter])\n",
    "\n",
    "    X_Bar = [1.0, np.mean(ip)]\n",
    "    \n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    while iteration < max_iter:\n",
    "        cost[iteration] = compute_cost(ip, op, params)\n",
    "        params_store[:, iteration] = params\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {iteration}')\n",
    "        print(f'cost: {cost[iteration]}')\n",
    "        \n",
    "        \n",
    "        # Apply batch gradient descent\n",
    "        for i in range(0, num_samples):\n",
    "            for j in range(len(params)):\n",
    "                params[j] += (alpha / num_samples) * (op[i] - np.dot(params, [1.0, ip[i]])) * X_Bar[j]\n",
    "        iteration += 1\n",
    "    \n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qbjhyZ71kq_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13718305.123371799\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 103878.31884215513\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 19266.25247873037\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 17313.52323875606\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 17199.355710496064\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 17191.779660394423\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 17191.27228163513\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 17191.238280722755\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 17191.23600212909\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 17191.235849427198\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 17191.23583919374\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 17191.23583850798\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 17191.23583846198\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 17191.235838458924\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 17191.235838458695\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 17191.235838458673\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 17191.23583845869\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 17191.23583845869\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 17191.23583845869\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 17191.235838458688\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 17191.235838458684\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 17191.235838458684\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 17191.23583845868\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 17191.235838458677\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 17191.235838458648\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 17191.235838458673\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 17191.235838458673\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 17191.235838458673\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 17191.235838458673\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 17191.23583845867\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 17191.235838458666\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 17191.235838458666\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 17191.235838458662\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 17191.23583845864\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 17191.235838458648\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 17191.235838458662\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 17191.235838458662\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 17191.235838458655\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 17191.235838458648\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 17191.235838458633\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 17191.23583845863\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 17191.23583845863\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 17191.235838458626\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 17191.235838458626\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 17191.23583845864\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 17191.23583845864\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 17191.23583845864\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 17191.235838458637\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 17191.235838458637\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 17191.235838458633\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 17191.235838458622\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 17191.235838458608\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 17191.235838458604\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 17191.235838458622\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 17191.235838458622\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 17191.23583845862\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 17191.23583845861\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 17191.235838458608\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 17191.235838458615\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 17191.235838458615\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 17191.23583845859\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 17191.23583845859\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 17191.235838458608\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 17191.235838458608\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 17191.2358384586\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 17191.235838458597\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 17191.235838458597\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 17191.235838458586\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 17191.235838458586\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 17191.23583845858\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 17191.23583845858\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 17191.23583845859\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 17191.23583845859\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 17191.235838458582\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 17191.235838458582\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 17191.23583845858\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 17191.235838458564\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 17191.235838458564\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 17191.235838458557\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 17191.235838458557\n",
      "--------------------------\n",
      "iteration: 80\n",
      "cost: 17191.235838458575\n",
      "--------------------------\n",
      "iteration: 81\n",
      "cost: 17191.23583845857\n",
      "--------------------------\n",
      "iteration: 82\n",
      "cost: 17191.235838458568\n",
      "--------------------------\n",
      "iteration: 83\n",
      "cost: 17191.235838458568\n",
      "--------------------------\n",
      "iteration: 84\n",
      "cost: 17191.235838458568\n",
      "--------------------------\n",
      "iteration: 85\n",
      "cost: 17191.235838458546\n",
      "--------------------------\n",
      "iteration: 86\n",
      "cost: 17191.235838458546\n",
      "--------------------------\n",
      "iteration: 87\n",
      "cost: 17191.23583845854\n",
      "--------------------------\n",
      "iteration: 88\n",
      "cost: 17191.23583845854\n",
      "--------------------------\n",
      "iteration: 89\n",
      "cost: 17191.23583845856\n",
      "--------------------------\n",
      "iteration: 90\n",
      "cost: 17191.23583845856\n",
      "--------------------------\n",
      "iteration: 91\n",
      "cost: 17191.23583845856\n",
      "--------------------------\n",
      "iteration: 92\n",
      "cost: 17191.23583845856\n",
      "--------------------------\n",
      "iteration: 93\n",
      "cost: 17191.235838458557\n",
      "--------------------------\n",
      "iteration: 94\n",
      "cost: 17191.235838458557\n",
      "--------------------------\n",
      "iteration: 95\n",
      "cost: 17191.235838458557\n",
      "--------------------------\n",
      "iteration: 96\n",
      "cost: 17191.235838458553\n",
      "--------------------------\n",
      "iteration: 97\n",
      "cost: 17191.23583845855\n",
      "--------------------------\n",
      "iteration: 98\n",
      "cost: 17191.23583845856\n",
      "--------------------------\n",
      "iteration: 99\n",
      "cost: 17191.23583845857\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "# Training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
    "\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "\n",
    "alpha_batch = 1e-3\n",
    "max_iter = 100\n",
    "params_hat_batch, cost_batch, params_store_batch =\\\n",
    "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Implement Stochastic Gradient Descent from scratch. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEIJL-WGkq_I"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "Algorithm can be given as follows:\n",
    "```shuffle(x, y)\n",
    "for i in 0 -> m:\n",
    "    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Gx9LN0wQkq_I"
   },
   "outputs": [],
   "source": [
    "# I changed input_var, output_var to ip, op as it was not taking the parameter values but taking the input_var and output_var (the whole data) defined in the earlier block.\n",
    "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using stochastic gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(num_samples)\n",
    "    params_store = np.zeros([2, num_samples])\n",
    "\n",
    "    X_Bar = [1.0, np.mean(ip)]\n",
    "    print(X_Bar)\n",
    "\n",
    "    i = 0\n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    for x, y in zip(ip, op):\n",
    "        cost[i] = compute_cost(ip, op, params)\n",
    "        params_store[:, i] = params\n",
    "\n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {i}')\n",
    "        print(f'cost: {cost[i]}')\n",
    "\n",
    "        # Apply stochastic gradient descent\n",
    "        for j in range(len(params)):\n",
    "            params[j] += (alpha / num_samples) * (y - np.dot(params, [1.0, x])) * X_Bar[j]\n",
    "        i+=1\n",
    "\n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HivE1gVkkq_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 51.4125]\n",
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13718305.123371799\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 13472327.948984515\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 13196543.06331085\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 12980489.185905684\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 11453978.310466517\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 10516300.347072113\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 9179844.067031952\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 8813777.129110653\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 8229187.137885486\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 7948290.883055335\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 7565723.750775049\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 6801720.013650015\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 6094163.217679422\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 5512972.767273023\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 5147847.136502948\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 4565113.998022179\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 4412951.249656013\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 3935663.537680433\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 3596797.2836399577\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 3392482.551909972\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 3271099.223727261\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 2892658.208827122\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 2738190.6985702426\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 2549237.5308850505\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 2272824.0066389022\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 2257446.036902007\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 2191061.2708967095\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 2101981.112470962\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 1834893.1920141205\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 1692758.9704781387\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 1572744.3591522267\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 1492008.9043803415\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 1423345.4671011919\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 1393061.0157089294\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 1335417.4599115867\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 1224214.8443294985\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 1145974.0707788994\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 1007143.4109719921\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 902345.6942645006\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 801484.264288619\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 789791.4628969136\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 793430.2710874768\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 719045.7036631422\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 695090.7783197431\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 681217.0833321817\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 608268.8479917941\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 607701.804751184\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 563577.7227429671\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 554581.1956871877\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 494404.4259039185\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 454401.43564737856\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 457342.73683706543\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 400134.1990791202\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 359970.5365792614\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 334016.5840615539\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 299115.1058365041\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 291577.8934388019\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 290477.29705673165\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 252427.029596585\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 236772.77724976494\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 228334.55615592765\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 206275.91585670944\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 184457.86204710125\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 177648.9020744204\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 168631.3024473068\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 172542.19935698784\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 174054.2331123312\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 162659.88448155826\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 160581.7838494315\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 166758.77237456478\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 161262.4472691853\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 157306.18508512055\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 151025.46670707158\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 152710.4441064505\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 151524.78064467647\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 142014.92555523795\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 130154.88772768663\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 117525.36049149973\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 111117.08001676777\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 108385.87536087264\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "alpha = 1e-3\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "params_hat, cost, params_store =\\\n",
    "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error for Batch Gradient Descent: 114.50511828167164\n",
      "Root Mean Square Error for Stochastic Gradient Descent: 262.0726521914285\n"
     ]
    }
   ],
   "source": [
    "# Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm\n",
    "def calcRMSE(X, Y, params):\n",
    "    Y_cap = np.zeros(len(Y))\n",
    "    for i in range(0, len(Y)):\n",
    "        Y_cap[i] = np.dot(params, [1.0, X[i]])\n",
    "    E = Y - Y_cap\n",
    "    return np.sqrt(np.sum(E*E)/len(E))\n",
    "\n",
    "print(\"Root Mean Square Error for Batch Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat_batch)))\n",
    "print(\"Root Mean Square Error for Stochastic Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1630902274461,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "930loAL6kq_L",
    "outputId": "e807576e-9852-4857-9a29-d367f2e0b26b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min cost with BGD: 17191.23583845854\n",
      "min cost with SGD: 108385.87536087264\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
    "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('normalized cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
    "print(f'min cost with SGD: {np.min(cost)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrpju6Kwkq_N"
   },
   "source": [
    "### Q2.4 Which linear regression model do you think works best for this data? Explain in brief. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbTux39kq_N"
   },
   "source": [
    "Based on how data is created, we have:\n",
    "\\begin{equation}\n",
    "y = 15 x + 2.4 + 300.0 * uniform(0, 1)\n",
    "\\end{equation}\n",
    "Thus, the model that works best for this data would be the one that provides the average results.\n",
    "The expected value of $uniform(0, 1)$ is $0.5$. Thus we have:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y &= 15 x + 2.4 + 300.0 * E(uniform(0, 1))\\\\\n",
    "y &= 15 x + 2.4 + 300 * (0.5)\\\\\n",
    "y &= 152.4 + 15x\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This will fit best with data and hence the best linear regression model.\n",
    "Comparing it with $y = \\theta_0 + \\theta_1 x$, we have $\\theta_0 = 152.4$ and $\\theta_1 = 15$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p02LYZrkq_N"
   },
   "source": [
    "### Q3. Linear Regression Analytical Problem (10 points)\n",
    "Consider the following training data.\n",
    "\n",
    "| X1 | X2 | Y |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1.5 |\n",
    "| 1 | 0 | 2 |\n",
    "| 1 | 1 | 2.5 |\n",
    "Suppose the data comes from a model y = $_{0}$ +$_{1}$x1 +$_{2}$x2 for unknown constants $_{0}$,$_{1}$,$_{2}$. Use least squares linear regression to find an estimate of $_{0}$,$_{1}$,$_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have:\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "        1 & 0 & 0\\\\\n",
    "        1 & 0 & 1\\\\\n",
    "        1 & 1 & 0\\\\\n",
    "        1 & 1 & 1\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Y = \\begin{bmatrix}\n",
    "        0\\\\\n",
    "        1.5\\\\\n",
    "        2\\\\\n",
    "        2.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "X^T = \\begin{bmatrix}\n",
    "        1 & 1 & 1 & 1\\\\\n",
    "        0 & 0 & 1 & 1\\\\\n",
    "        0 & 1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence,\n",
    "\\begin{equation}\n",
    "X^TX = \\begin{bmatrix}\n",
    "        4 & 2 & 2\\\\\n",
    "        2 & 2 & 1\\\\\n",
    "        2 & 1 & 2\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "|X^TX| = 4(4-1) -2(4-2)+2(2-4)=4\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Cf(X^TX) = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Adj(X^TX) = (Cf(X^TX))^T = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{|X^TX|} Adj(X^TX)= \\begin{bmatrix}\n",
    "        0.75 & -0.5 & -0.5\\\\\n",
    "        -0.5 & 1 & 0\\\\\n",
    "        -0.5 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1}X^T = \\begin{bmatrix}\n",
    "        0.75 & 0.25 & 0.25 & -0.25\\\\\n",
    "        -0.5 & -0.5 & 0.5 & 0.5\\\\\n",
    "        -0.5 & 0.5 & -0.5 & 0.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We have, $\\theta^* = (X^TX)^{-1}X^TY$\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "\\theta^* = \\begin{bmatrix}\n",
    "        \\theta_0\\\\\n",
    "        \\theta_1\\\\\n",
    "        \\theta_2\n",
    "    \\end{bmatrix}\n",
    "    =\\begin{bmatrix}\n",
    "        0.25\\\\\n",
    "        1.5\\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the linear regression equation becomes:\n",
    "\\begin{equation}\n",
    "y = 0.25 + 1.5X_1 + X_2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-1-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}