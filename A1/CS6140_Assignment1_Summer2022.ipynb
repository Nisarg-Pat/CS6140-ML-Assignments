{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1avDQgkq-5"
   },
   "source": [
    "# CS 6140 Machine Learning: Assignment - 1 (Total Points: 100)\n",
    "## Prof. Ahmad Uzair "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFpvDelkq-6"
   },
   "source": [
    "### Q1. Decision Tree Classifier (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Growing Decison Trees from scratch (40 points)\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "You must also print the Decision Tree. Use information gain based on entropy as the splitting measure.\n",
    "\n",
    "Use the data.csv dataset for this particular question. The dataset should be uploaded on Canvas with Assignment 1. Split the dataset into training and test data and calculate testing accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Utils class to perform certain calculations\n",
    "class Utils:\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)\n",
    "\n",
    "\n",
    "# Node of a DecisionTree. Can be either regular node or leaf node.\n",
    "# Normal node contains information about the feature and the value it compares in that node.\n",
    "# Leaf node contains the type of class.\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, X, Y):\n",
    "        if len(X) == 0:\n",
    "            return\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.isLeaf = False\n",
    "        self.classType = -1\n",
    "        self.H = self.entropy(Y)\n",
    "        self.trueChild = None\n",
    "        self.falseChild = None\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        for i in range(len(freq)):\n",
    "            if freq[i] == len(Y):\n",
    "                self.isLeaf = True\n",
    "                self.classType = num[i]\n",
    "                return\n",
    "\n",
    "        self.featureIndex, self.compValue = self.findBestSplit()\n",
    "        tx, ty, fx, fy = self.split(X, Y, self.featureIndex, self.compValue)\n",
    "        self.trueChild = DecisionTreeNode(tx, ty)\n",
    "        self.falseChild = DecisionTreeNode(fx, fy)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        \"\"\"\n",
    "        Calculates Entropy of an array.\n",
    "        :param Y: Array\n",
    "        :return: Entropy of the array\n",
    "        \"\"\"\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        h = 0.0\n",
    "        for val in freq:\n",
    "            if val != 0:\n",
    "                prob = val / len(Y)\n",
    "                h -= prob * (np.log2(prob))\n",
    "        return h\n",
    "\n",
    "    def informationGain(self, X, Y, featureIndex, value):\n",
    "        \"\"\"\n",
    "        Calculates Information Gain based on split featureIndex <= value\n",
    "        :param X: feature values of samples\n",
    "        :param Y: class of samples\n",
    "        :param featureIndex: the index of feature on which to split\n",
    "        :param value: the value to compare X[featureIndex] with\n",
    "        :return: Information Gain by the split\n",
    "        \"\"\"\n",
    "\n",
    "        tx, ty, fx, fy = self.split(X, Y, featureIndex, value)\n",
    "        expectedEntropy = 0\n",
    "        expectedEntropy += (len(ty) / len(Y)) * self.entropy(ty)\n",
    "        expectedEntropy += (len(fy) / len(Y)) * self.entropy(fy)\n",
    "        IG = self.H - expectedEntropy\n",
    "        return IG\n",
    "\n",
    "    def split(self, X, Y, featureIndex, value):\n",
    "        \"\"\"\n",
    "        Splits X and Y based on X[featureIndex] <= value into two arrays.\n",
    "        :param X: feature values of samples\n",
    "        :param Y: class of samples\n",
    "        :param featureIndex: the index of feature on which to split\n",
    "        :param value: the value to compare X[featureIndex] with\n",
    "        :return: the split of X and Y based on the condition\n",
    "        \"\"\"\n",
    "\n",
    "        tx, ty, fx, fy = [], [], [], []\n",
    "        for i in range(0, len(X)):\n",
    "            if X[i][featureIndex] < value:\n",
    "                tx.append(X[i])\n",
    "                ty.append(Y[i])\n",
    "            else:\n",
    "                fx.append(X[i])\n",
    "                fy.append(Y[i])\n",
    "        return np.array(tx), np.array(ty), np.array(fx), np.array(fy)\n",
    "\n",
    "    def findBestSplit(self):\n",
    "        \"\"\"\n",
    "        Finds the best split based on Information Gain.\n",
    "        Values are selected as midpoints of adjecent feature values in sorted order\n",
    "        :return: The featureIndex and the value which gives the best split\n",
    "        \"\"\"\n",
    "        copy_X = np.transpose(self.X)\n",
    "        maxIG = float(\"-inf\")\n",
    "        bestFeatureIndex = None\n",
    "        bestValue = None\n",
    "        for i in range(0, len(copy_X)):\n",
    "            T = np.sort(copy_X[i])\n",
    "            for j in range(1, len(T)):\n",
    "                midValue = (T[j - 1] + T[j]) / 2.0\n",
    "                currentIG = self.informationGain(self.X, self.Y, i, midValue)\n",
    "                if currentIG > maxIG:\n",
    "                    maxIG = currentIG\n",
    "                    bestFeatureIndex = i\n",
    "                    bestValue = midValue\n",
    "        return bestFeatureIndex, bestValue\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class based on the generated decision nodes\n",
    "        :param X: features of a sample\n",
    "        :return: the predicted class of the sample\n",
    "        \"\"\"\n",
    "\n",
    "        if self.isLeaf:\n",
    "            return self.classType\n",
    "        elif X[self.featureIndex] <= self.compValue:\n",
    "            return self.trueChild.predict(X)\n",
    "        else:\n",
    "            return self.falseChild.predict(X)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        String representation of the node\n",
    "        :return: String representation of the node\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isLeaf:\n",
    "            return \"class:\" + str(self.classType)\n",
    "        else:\n",
    "            return \"feature\" + str(self.featureIndex + 1) + \" <= \" + str(self.compValue)\n",
    "\n",
    "\n",
    "#The Decision Tree Class\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Learns the decision tree based on the given features and classes\n",
    "        :param X: feature values of a sample\n",
    "        :param Y: classes of the samples\n",
    "        \"\"\"\n",
    "        self.root = DecisionTreeNode(X, Y)\n",
    "\n",
    "    def print(self, feature_names, class_names):\n",
    "        \"\"\"\n",
    "        Prints the decision tree as in sklearn's export_text.\n",
    "        :param feature_names: names of the features\n",
    "        :param class_names: Names of the classes\n",
    "        \"\"\"\n",
    "        self.preOrder(self.root, feature_names, class_names, \"|--- \")\n",
    "\n",
    "    def preOrder(self, root, feature_names, class_names, prev):\n",
    "        \"\"\"\n",
    "        Helper function to print.\n",
    "        \"\"\"\n",
    "        if root == None:\n",
    "            return\n",
    "        if root.isLeaf:\n",
    "            print(prev + \"class: \" + class_names[root.classType])\n",
    "            return\n",
    "        print(prev + feature_names[root.featureIndex] + \" <= \" + str(root.compValue))\n",
    "        self.preOrder(root.trueChild, feature_names, class_names, \"|   \" + prev)\n",
    "        print(prev + feature_names[root.featureIndex] + \" >  \" + str(root.compValue))\n",
    "        self.preOrder(root.falseChild, feature_names, class_names, \"|   \" + prev)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the classes of samples\n",
    "        :param X: features of samples\n",
    "        :return: the predicted class of every samples\n",
    "        \"\"\"\n",
    "        Y = []\n",
    "        for i in range(len(X)):\n",
    "            Y.append(self.root.predict(X[i]))\n",
    "        return np.array(Y)\n",
    "\n",
    "    def accuracy(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Returns the accuracy of the decision tree\n",
    "        :param Y: The actual class of samples\n",
    "        :param Y_hat: The predicted class of samples\n",
    "        :return: The accuracy of the decision tree\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature4 <= 1.75\n",
      "|   |   |--- feature3 <= 5.35\n",
      "|   |   |   |--- feature1 <= 4.95\n",
      "|   |   |   |   |--- feature2 <= 2.45\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature2 >  2.45\n",
      "|   |   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature1 >  4.95\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  5.35\n",
      "|   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.75\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature1 <= 5.95\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  5.95\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "Testing Accuracy: 94.74\n"
     ]
    }
   ],
   "source": [
    "# Reads the data, splits it and fits a decision tree based on training data.\n",
    "# Accuracy is measured on testing data.\n",
    "data = pandas.read_csv(\"data.csv\")\n",
    "X = data.drop(\"class\", axis=1).values\n",
    "Y = data[\"class\"].values\n",
    "\n",
    "feature_names = list(data.drop(\"class\", axis=1).columns)\n",
    "class_names = [str(i) for i in range(0, len(set(Y)))]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "dt.print(feature_names, class_names)\n",
    "\n",
    "Y_test_pred = dt.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Decision Tree using Sklearn Library (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Decision Tree Classifier from the Sklearn Library and use gini index as a splitting measure. Use the data.csv dataset.\n",
    "Calculate accuracy for this model. \n",
    "Print the Decision tree and compare the Decision Trees generated from your code and Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature4 <= 1.75\n",
      "|   |   |--- feature3 <= 5.35\n",
      "|   |   |   |--- feature4 <= 1.65\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature4 >  1.65\n",
      "|   |   |   |   |--- feature3 <= 4.75\n",
      "|   |   |   |   |   |--- class: 2\n",
      "|   |   |   |   |--- feature3 >  4.75\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  5.35\n",
      "|   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.75\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature1 <= 5.95\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  5.95\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "\n",
      "Testing Accuracy: 94.74\n"
     ]
    }
   ],
   "source": [
    "# Reads the data, splits it and fits sklearn.tree.DecisionTreeClassifier based on training data.\n",
    "# Accuracy is measured on testing data.\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(export_text(clf, feature_names=feature_names))\n",
    "\n",
    "Y_test_pred = clf.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Comparision of the two generated decision trees:\n",
    "\n",
    "Both the trees generated are almost identical most of the time.\n",
    "The difference in the trees could occur because of the different splitting measure (Information Gain for my code and Gini for sklearn code). The difference is still minimal and the accuracy achieved by both of the trees is similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEBH56pukq_H"
   },
   "source": [
    "### Q2 Linear Regression (40 points)\n",
    "\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FL1tuQEkq_H"
   },
   "source": [
    "## Gradient descent algorithm \n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This minimizes the following cost function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "h(x_i) = \\theta^T \\bar{x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "cOem1EjQkq_H"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "true_slope = 15\n",
    "true_intercept = 2.4\n",
    "input_var = np.arange(0.0,100.0)\n",
    "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1630902228487,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "SNvDqYEykq_H",
    "outputId": "c7f53823-73d9-473f-9e28-944f1f09a415",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.scatter(input_var, output_var)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "XgNhbpEmkq_I"
   },
   "outputs": [],
   "source": [
    "def compute_cost(ip, op, params):\n",
    "    \"\"\"\n",
    "    Cost function in linear regression where the cost is calculated\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    num_samples = len(ip)\n",
    "    cost_sum = 0.0\n",
    "    for x,y in zip(ip, op):\n",
    "        y_hat = np.dot(params, np.array([1.0, x]))\n",
    "        cost_sum += (y_hat - y) ** 2\n",
    "    \n",
    "    cost = cost_sum / (num_samples)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Implement Linear Regression using Batch Gradient Descent from scratch.  (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ao7aYu9kq_I"
   },
   "source": [
    "\n",
    "### Batch gradient descent\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "f0z876gDkq_I"
   },
   "outputs": [],
   "source": [
    "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(max_iter)\n",
    "    params_store = np.zeros([2, max_iter])\n",
    "\n",
    "    x = np.ones([num_samples, len(params)])\n",
    "    for i in range(num_samples):\n",
    "        x[i][1] = ip[i]\n",
    "    \n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    while iteration < max_iter:\n",
    "        cost[iteration] = compute_cost(ip, op, params)\n",
    "        params_store[:, iteration] = params\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {iteration}')\n",
    "        print(f'cost: {cost[iteration]}')\n",
    "\n",
    "\n",
    "        \n",
    "        # Apply batch gradient descent\n",
    "        for i in range(0, num_samples):\n",
    "            prevParams = params\n",
    "            for j in range(len(params)):\n",
    "                params[j] += (alpha / num_samples) * (op[i] - np.dot(prevParams, x[i])) * x[i][j]\n",
    "        iteration += 1\n",
    "    \n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "qbjhyZ71kq_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 12430652.090655794\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 29261.632032319154\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 13152.575662741016\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 13238.219880184206\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 13240.184661806445\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 13237.7775161004\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 13235.198894536214\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 13232.614849457901\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 13230.031880771354\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 13227.450242168654\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 13224.869942909932\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 13222.290982690407\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 13219.71336083145\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 13217.137076640032\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 13214.562129423022\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 13211.988518487531\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 13209.416243141095\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 13206.845302691581\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 13204.275696447221\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 13201.70742371657\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 13199.140483808596\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 13196.57487603261\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 13194.010599698257\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 13191.447654115571\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 13188.886038594888\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 13186.325752446972\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 13183.766794982901\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 13181.209165514123\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 13178.652863352449\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 13176.09788781002\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 13173.54423819936\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 13170.991913833332\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 13168.44091402517\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 13165.891238088449\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 13163.342885337112\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 13160.795855085442\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 13158.2501466481\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 13155.705759340086\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 13153.162692476748\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 13150.620945373801\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 13148.080517347324\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 13145.541407713721\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 13143.003615789776\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 13140.467140892604\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 13137.931982339704\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 13135.398139448898\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 13132.865611538382\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 13130.334397926708\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 13127.804497932739\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 13125.27591087575\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 13122.748636075321\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 13120.22267285144\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 13117.69802052437\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 13115.17467841478\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 13112.65264584369\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 13110.131922132461\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 13107.61250660278\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 13105.09439857674\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 13102.577597376705\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 13100.062102325512\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 13097.547912746195\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 13095.03502796229\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 13092.523447297579\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 13090.013170076241\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 13087.504195622772\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 13084.996523262054\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 13082.49015231931\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 13079.98508212009\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 13077.481311990303\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 13074.978841256241\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 13072.477669244463\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 13069.977795281984\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 13067.479218696093\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 13064.981938814453\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 13062.485954965065\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 13059.991266476269\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 13057.497872676786\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 13055.00577289568\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 13052.514966462319\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 13050.025452706443\n",
      "--------------------------\n",
      "iteration: 80\n",
      "cost: 13047.53723095816\n",
      "--------------------------\n",
      "iteration: 81\n",
      "cost: 13045.050300547908\n",
      "--------------------------\n",
      "iteration: 82\n",
      "cost: 13042.56466080648\n",
      "--------------------------\n",
      "iteration: 83\n",
      "cost: 13040.080311064976\n",
      "--------------------------\n",
      "iteration: 84\n",
      "cost: 13037.597250654911\n",
      "--------------------------\n",
      "iteration: 85\n",
      "cost: 13035.11547890809\n",
      "--------------------------\n",
      "iteration: 86\n",
      "cost: 13032.634995156692\n",
      "--------------------------\n",
      "iteration: 87\n",
      "cost: 13030.155798733213\n",
      "--------------------------\n",
      "iteration: 88\n",
      "cost: 13027.677888970537\n",
      "--------------------------\n",
      "iteration: 89\n",
      "cost: 13025.201265201864\n",
      "--------------------------\n",
      "iteration: 90\n",
      "cost: 13022.725926760739\n",
      "--------------------------\n",
      "iteration: 91\n",
      "cost: 13020.251872981098\n",
      "--------------------------\n",
      "iteration: 92\n",
      "cost: 13017.779103197103\n",
      "--------------------------\n",
      "iteration: 93\n",
      "cost: 13015.307616743421\n",
      "--------------------------\n",
      "iteration: 94\n",
      "cost: 13012.83741295493\n",
      "--------------------------\n",
      "iteration: 95\n",
      "cost: 13010.368491166959\n",
      "--------------------------\n",
      "iteration: 96\n",
      "cost: 13007.900850715054\n",
      "--------------------------\n",
      "iteration: 97\n",
      "cost: 13005.43449093526\n",
      "--------------------------\n",
      "iteration: 98\n",
      "cost: 13002.969411163846\n",
      "--------------------------\n",
      "iteration: 99\n",
      "cost: 13000.505610737457\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "# Training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
    "\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "\n",
    "alpha_batch = 1e-3\n",
    "max_iter = 100\n",
    "params_hat_batch, cost_batch, params_store_batch =\\\n",
    "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Implement Stochastic Gradient Descent from scratch. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEIJL-WGkq_I"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "Algorithm can be given as follows:\n",
    "```shuffle(x, y)\n",
    "for i in 0 -> m:\n",
    "    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "Gx9LN0wQkq_I"
   },
   "outputs": [],
   "source": [
    "# I changed input_var, output_var to ip, op as it was not taking the parameter values but taking the input_var and output_var (the whole data) defined in the earlier block.\n",
    "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using stochastic gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(num_samples)\n",
    "    params_store = np.zeros([2, num_samples])\n",
    "\n",
    "    i = 0\n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    for x, y in zip(ip, op):\n",
    "        cost[i] = compute_cost(ip, op, params)\n",
    "        params_store[:, i] = params\n",
    "\n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {i}')\n",
    "        print(f'cost: {cost[i]}')\n",
    "\n",
    "        # Apply stochastic gradient descent\n",
    "        X = [1.0, x]\n",
    "        prevParams = params\n",
    "        for j in range(len(params)):\n",
    "            params[j] += (alpha / num_samples) * (y - np.dot(prevParams, X)) * X[j]\n",
    "        i+=1\n",
    "\n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "HivE1gVkkq_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 12430652.090655794\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 11440461.93770192\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 11439568.531020656\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 11251431.27591355\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 11220332.431283709\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 11215977.864233727\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 10872136.417734178\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 8717779.741036536\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 7717650.632275736\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 6727506.45710447\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 6478120.641898992\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 5690406.655280794\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 5130796.459059219\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 4142946.2551400065\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 4069367.7837943584\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 3325772.6607538457\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 3326367.6037715483\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 3321951.9589792048\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 2968830.871016655\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 2954342.8996349694\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 2799865.3599559995\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 2388096.068659636\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 2387806.438413036\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 2237880.363102248\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 2026696.1603449932\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 1897950.9505753678\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 1590596.8284736439\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 1568190.9557215194\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 1403119.7468800943\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 1143215.660253732\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 1055135.5164627847\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 1056154.127429601\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 1013037.2727688119\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 936965.921502792\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 929509.682634923\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 806715.2831843279\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 806718.3195401418\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 778617.3679707067\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 765684.614674379\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 763435.5321494162\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 754879.7438967382\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 581612.965781467\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 579258.5514013693\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 572148.1476258397\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 534870.9136751868\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 484712.0189984307\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 415494.9247368012\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 413753.7642209368\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 338834.31265274016\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 295099.8320326821\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 272742.7833811649\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 235625.48009034788\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 222985.59190957397\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 191696.29054782243\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 188442.4717650363\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 185348.8812162489\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 145790.28498544195\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 146902.07498245552\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 115549.27231320113\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 116416.39166374927\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 111903.57511568705\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 112788.73900994255\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 84620.93004684782\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 81857.71597991498\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 78144.5483086422\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 78305.76877303264\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 76112.9694311665\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 68566.44214700072\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 68224.1239731674\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 64295.50555585569\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 56558.18318079515\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 55760.36483443967\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 55818.245450115195\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 46295.981279737534\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 46274.76363744363\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 39877.52428090619\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 40454.00748465689\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 35165.337297429054\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 30108.910397649695\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 30269.215794971136\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "alpha = 1e-3\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "params_hat, cost, params_store =\\\n",
    "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error for Batch Gradient Descent: 118.19659408694272\n",
      "Root Mean Square Error for Stochastic Gradient Descent: 177.67061026434854\n"
     ]
    }
   ],
   "source": [
    "# Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm\n",
    "def calcRMSE(X, Y, params):\n",
    "    \"\"\"\n",
    "    Calculates the Root mean square error.\n",
    "    :param X: The input varaible of samples\n",
    "    :param Y: The actual output variable of samples.\n",
    "    :param params: The parameters of linears regression\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    Y_cap = np.zeros(len(Y))\n",
    "    for i in range(0, len(Y)):\n",
    "        Y_cap[i] = np.dot(params, [1.0, X[i]])\n",
    "    E = Y - Y_cap\n",
    "    return np.sqrt(np.sum(E*E)/len(E))\n",
    "\n",
    "print(\"Root Mean Square Error for Batch Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat_batch)))\n",
    "print(\"Root Mean Square Error for Stochastic Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1630902274461,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "930loAL6kq_L",
    "outputId": "e807576e-9852-4857-9a29-d367f2e0b26b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min cost with BGD: 13000.505610737457\n",
      "min cost with SGD: 30108.910397649695\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
    "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('normalized cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
    "print(f'min cost with SGD: {np.min(cost)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrpju6Kwkq_N"
   },
   "source": [
    "### Q2.4 Which linear regression model do you think works best for this data? Explain in brief. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbTux39kq_N"
   },
   "source": [
    "Based on how data is created, we have:\n",
    "\\begin{equation}\n",
    "y = 15 x + 2.4 + 300.0 * uniform(0, 1)\n",
    "\\end{equation}\n",
    "Thus, the model that works best for this data would be the one that provides the average results.\n",
    "The expected value of $uniform(0, 1)$ is $0.5$. Thus we have:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y &= 15 x + 2.4 + 300.0 * E(uniform(0, 1))\\\\\n",
    "y &= 15 x + 2.4 + 300 * (0.5)\\\\\n",
    "y &= 152.4 + 15x\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This will fit best with data and hence the best linear regression model.\n",
    "Comparing it with $y = \\theta_0 + \\theta_1 x$, we have $\\theta_0 = 152.4$ and $\\theta_1 = 15$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p02LYZrkq_N"
   },
   "source": [
    "### Q3. Linear Regression Analytical Problem (10 points)\n",
    "Consider the following training data.\n",
    "\n",
    "| X1 | X2 | Y |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1.5 |\n",
    "| 1 | 0 | 2 |\n",
    "| 1 | 1 | 2.5 |\n",
    "Suppose the data comes from a model y = $θ_{0}$ +$θ_{1}$x1 +$θ_{2}$x2 for unknown constants $θ_{0}$,$θ_{1}$,$θ_{2}$. Use least squares linear regression to find an estimate of $θ_{0}$,$θ_{1}$,$θ_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have:\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "        1 & 0 & 0\\\\\n",
    "        1 & 0 & 1\\\\\n",
    "        1 & 1 & 0\\\\\n",
    "        1 & 1 & 1\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Y = \\begin{bmatrix}\n",
    "        0\\\\\n",
    "        1.5\\\\\n",
    "        2\\\\\n",
    "        2.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "X^T = \\begin{bmatrix}\n",
    "        1 & 1 & 1 & 1\\\\\n",
    "        0 & 0 & 1 & 1\\\\\n",
    "        0 & 1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence,\n",
    "\\begin{equation}\n",
    "X^TX = \\begin{bmatrix}\n",
    "        4 & 2 & 2\\\\\n",
    "        2 & 2 & 1\\\\\n",
    "        2 & 1 & 2\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "|X^TX| = 4(4-1) -2(4-2)+2(2-4)=4\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Cf(X^TX) = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Adj(X^TX) = (Cf(X^TX))^T = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{|X^TX|} Adj(X^TX)= \\begin{bmatrix}\n",
    "        0.75 & -0.5 & -0.5\\\\\n",
    "        -0.5 & 1 & 0\\\\\n",
    "        -0.5 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1}X^T = \\begin{bmatrix}\n",
    "        0.75 & 0.25 & 0.25 & -0.25\\\\\n",
    "        -0.5 & -0.5 & 0.5 & 0.5\\\\\n",
    "        -0.5 & 0.5 & -0.5 & 0.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We have, $\\theta^* = (X^TX)^{-1}X^TY$\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "\\theta^* = \\begin{bmatrix}\n",
    "        \\theta_0\\\\\n",
    "        \\theta_1\\\\\n",
    "        \\theta_2\n",
    "    \\end{bmatrix}\n",
    "    =\\begin{bmatrix}\n",
    "        0.25\\\\\n",
    "        1.5\\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the linear regression equation becomes:\n",
    "\\begin{equation}\n",
    "y = 0.25 + 1.5X_1 + X_2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-1-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}