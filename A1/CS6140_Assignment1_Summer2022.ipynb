{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1avDQgkq-5"
   },
   "source": [
    "# CS 6140 Machine Learning: Assignment - 1 (Total Points: 100)\n",
    "## Prof. Ahmad Uzair "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFpvDelkq-6"
   },
   "source": [
    "### Q1. Decision Tree Classifier (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Growing Decison Trees from scratch (40 points)\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "You must also print the Decision Tree. Use information gain based on entropy as the splitting measure.\n",
    "\n",
    "Use the data.csv dataset for this particular question. The dataset should be uploaded on Canvas with Assignment 1. Split the dataset into training and test data and calculate testing accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Utils class to perform certain calculations\n",
    "class Utils:\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)\n",
    "\n",
    "\n",
    "# Node of a DecisionTree. Can be either regular node or leaf node.\n",
    "# Normal node contains information about the feature and the value it compares in that node.\n",
    "# Leaf node contains the type of class.\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, X, Y):\n",
    "        if len(X) == 0:\n",
    "            return\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.isLeaf = False\n",
    "        self.classType = -1\n",
    "        self.H = self.entropy(Y)\n",
    "        self.trueChild = None\n",
    "        self.falseChild = None\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        for i in range(len(freq)):\n",
    "            if freq[i] == len(Y):\n",
    "                self.isLeaf = True\n",
    "                self.classType = num[i]\n",
    "                return\n",
    "\n",
    "        self.featureIndex, self.compValue = self.findBestSplit()\n",
    "        tx, ty, fx, fy = self.split(X, Y, self.featureIndex, self.compValue)\n",
    "        self.trueChild = DecisionTreeNode(tx, ty)\n",
    "        self.falseChild = DecisionTreeNode(fx, fy)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        \"\"\"\n",
    "        Calculates Entropy of an array.\n",
    "        :param Y: Array\n",
    "        :return: Entropy of the array\n",
    "        \"\"\"\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        h = 0.0\n",
    "        for val in freq:\n",
    "            if val != 0:\n",
    "                prob = val / len(Y)\n",
    "                h -= prob * (np.log2(prob))\n",
    "        return h\n",
    "\n",
    "    def informationGain(self, X, Y, featureIndex, value):\n",
    "        \"\"\"\n",
    "        Calculates Information Gain based on split featureIndex <= value\n",
    "        :param X: feature values of samples\n",
    "        :param Y: class of samples\n",
    "        :param featureIndex: the index of feature on which to split\n",
    "        :param value: the value to compare X[featureIndex] with\n",
    "        :return: Information Gain by the split\n",
    "        \"\"\"\n",
    "\n",
    "        tx, ty, fx, fy = self.split(X, Y, featureIndex, value)\n",
    "        expectedEntropy = 0\n",
    "        expectedEntropy += (len(ty) / len(Y)) * self.entropy(ty)\n",
    "        expectedEntropy += (len(fy) / len(Y)) * self.entropy(fy)\n",
    "        IG = self.H - expectedEntropy\n",
    "        return IG\n",
    "\n",
    "    def split(self, X, Y, featureIndex, value):\n",
    "        \"\"\"\n",
    "        Splits X and Y based on X[featureIndex] <= value into two arrays.\n",
    "        :param X: feature values of samples\n",
    "        :param Y: class of samples\n",
    "        :param featureIndex: the index of feature on which to split\n",
    "        :param value: the value to compare X[featureIndex] with\n",
    "        :return: the split of X and Y based on the condition\n",
    "        \"\"\"\n",
    "\n",
    "        tx, ty, fx, fy = [], [], [], []\n",
    "        for i in range(0, len(X)):\n",
    "            if X[i][featureIndex] < value:\n",
    "                tx.append(X[i])\n",
    "                ty.append(Y[i])\n",
    "            else:\n",
    "                fx.append(X[i])\n",
    "                fy.append(Y[i])\n",
    "        return np.array(tx), np.array(ty), np.array(fx), np.array(fy)\n",
    "\n",
    "    def findBestSplit(self):\n",
    "        \"\"\"\n",
    "        Finds the best split based on Information Gain.\n",
    "        Values are selected as midpoints of adjecent feature values in sorted order\n",
    "        :return: The featureIndex and the value which gives the best split\n",
    "        \"\"\"\n",
    "        copy_X = np.transpose(self.X)\n",
    "        maxIG = float(\"-inf\")\n",
    "        bestFeatureIndex = None\n",
    "        bestValue = None\n",
    "        for i in range(0, len(copy_X)):\n",
    "            T = np.sort(copy_X[i])\n",
    "            for j in range(1, len(T)):\n",
    "                midValue = (T[j - 1] + T[j]) / 2.0\n",
    "                currentIG = self.informationGain(self.X, self.Y, i, midValue)\n",
    "                if currentIG > maxIG:\n",
    "                    maxIG = currentIG\n",
    "                    bestFeatureIndex = i\n",
    "                    bestValue = midValue\n",
    "        return bestFeatureIndex, bestValue\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class based on the generated decision nodes\n",
    "        :param X: features of a sample\n",
    "        :return: the predicted class of the sample\n",
    "        \"\"\"\n",
    "\n",
    "        if self.isLeaf:\n",
    "            return self.classType\n",
    "        elif X[self.featureIndex] <= self.compValue:\n",
    "            return self.trueChild.predict(X)\n",
    "        else:\n",
    "            return self.falseChild.predict(X)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        String representation of the node\n",
    "        :return: String representation of the node\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isLeaf:\n",
    "            return \"class:\" + str(self.classType)\n",
    "        else:\n",
    "            return \"feature\" + str(self.featureIndex + 1) + \" <= \" + str(self.compValue)\n",
    "\n",
    "\n",
    "#The Decision Tree Class\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Learns the decision tree based on the given features and classes\n",
    "        :param X: feature values of a sample\n",
    "        :param Y: classes of the samples\n",
    "        \"\"\"\n",
    "        self.root = DecisionTreeNode(X, Y)\n",
    "\n",
    "    def print(self, feature_names, class_names):\n",
    "        \"\"\"\n",
    "        Prints the decision tree as in sklearn's export_text.\n",
    "        :param feature_names: names of the features\n",
    "        :param class_names: Names of the classes\n",
    "        \"\"\"\n",
    "        self.preOrder(self.root, feature_names, class_names, \"|--- \")\n",
    "\n",
    "    def preOrder(self, root, feature_names, class_names, prev):\n",
    "        \"\"\"\n",
    "        Helper function to print.\n",
    "        \"\"\"\n",
    "        if root == None:\n",
    "            return\n",
    "        if root.isLeaf:\n",
    "            print(prev + \"class: \" + class_names[root.classType])\n",
    "            return\n",
    "        print(prev + feature_names[root.featureIndex] + \" <= \" + str(root.compValue))\n",
    "        self.preOrder(root.trueChild, feature_names, class_names, \"|   \" + prev)\n",
    "        print(prev + feature_names[root.featureIndex] + \" >  \" + str(root.compValue))\n",
    "        self.preOrder(root.falseChild, feature_names, class_names, \"|   \" + prev)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the classes of samples\n",
    "        :param X: features of samples\n",
    "        :return: the predicted class of every samples\n",
    "        \"\"\"\n",
    "        Y = []\n",
    "        for i in range(len(X)):\n",
    "            Y.append(self.root.predict(X[i]))\n",
    "        return np.array(Y)\n",
    "\n",
    "    def accuracy(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Returns the accuracy of the decision tree\n",
    "        :param Y: The actual class of samples\n",
    "        :param Y_hat: The predicted class of samples\n",
    "        :return: The accuracy of the decision tree\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.7\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.7\n",
      "|   |--- feature4 <= 1.65\n",
      "|   |   |--- feature3 <= 4.95\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.95\n",
      "|   |   |   |--- feature1 <= 6.05\n",
      "|   |   |   |   |--- feature2 <= 2.45\n",
      "|   |   |   |   |   |--- class: 2\n",
      "|   |   |   |   |--- feature2 >  2.45\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  6.05\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.65\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature2 <= 3.1\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature2 >  3.1\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "Testing Accuracy: 97.37\n"
     ]
    }
   ],
   "source": [
    "# Reads the data, splits it and fits a decision tree based on training data.\n",
    "# Accuracy is measured on testing data.\n",
    "data = pandas.read_csv(\"data.csv\")\n",
    "X = data.drop(\"class\", axis=1).values\n",
    "Y = data[\"class\"].values\n",
    "\n",
    "feature_names = list(data.drop(\"class\", axis=1).columns)\n",
    "class_names = [str(i) for i in range(0, len(set(Y)))]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "dt.print(feature_names, class_names)\n",
    "\n",
    "Y_test_pred = dt.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Decision Tree using Sklearn Library (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Decision Tree Classifier from the Sklearn Library and use gini index as a splitting measure. Use the data.csv dataset.\n",
    "Calculate accuracy for this model. \n",
    "Print the Decision tree and compare the Decision Trees generated from your code and Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature4 <= 0.80\n",
      "|   |--- class: 0\n",
      "|--- feature4 >  0.80\n",
      "|   |--- feature4 <= 1.65\n",
      "|   |   |--- feature3 <= 4.95\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.95\n",
      "|   |   |   |--- feature3 <= 5.35\n",
      "|   |   |   |   |--- feature4 <= 1.55\n",
      "|   |   |   |   |   |--- class: 2\n",
      "|   |   |   |   |--- feature4 >  1.55\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature3 >  5.35\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.65\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature2 <= 3.10\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature2 >  3.10\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "\n",
      "Testing Accuracy: 97.37\n"
     ]
    }
   ],
   "source": [
    "# Reads the data, splits it and fits sklearn.tree.DecisionTreeClassifier based on training data.\n",
    "# Accuracy is measured on testing data.\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(export_text(clf, feature_names=feature_names))\n",
    "\n",
    "Y_test_pred = clf.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Comparision of the two generated decision trees:\n",
    "\n",
    "Both the trees generated are almost identical most of the time.\n",
    "The difference in the trees could occur because of the different splitting measure (Information Gain for my code and Gini for sklearn code). The difference is still minimal and the accuracy achieved by both of the trees is similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEBH56pukq_H"
   },
   "source": [
    "### Q2 Linear Regression (40 points)\n",
    "\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FL1tuQEkq_H"
   },
   "source": [
    "## Gradient descent algorithm \n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This minimizes the following cost function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "h(x_i) = \\theta^T \\bar{x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cOem1EjQkq_H"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "true_slope = 15\n",
    "true_intercept = 2.4\n",
    "input_var = np.arange(0.0,100.0)\n",
    "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1630902228487,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "SNvDqYEykq_H",
    "outputId": "c7f53823-73d9-473f-9e28-944f1f09a415",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.scatter(input_var, output_var)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XgNhbpEmkq_I"
   },
   "outputs": [],
   "source": [
    "def compute_cost(ip, op, params):\n",
    "    \"\"\"\n",
    "    Cost function in linear regression where the cost is calculated\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    num_samples = len(ip)\n",
    "    cost_sum = 0.0\n",
    "    for x,y in zip(ip, op):\n",
    "        y_hat = np.dot(params, np.array([1.0, x]))\n",
    "        cost_sum += (y_hat - y) ** 2\n",
    "    \n",
    "    cost = cost_sum / (num_samples)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Implement Linear Regression using Batch Gradient Descent from scratch.  (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ao7aYu9kq_I"
   },
   "source": [
    "\n",
    "### Batch gradient descent\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f0z876gDkq_I"
   },
   "outputs": [],
   "source": [
    "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(max_iter)\n",
    "    params_store = np.zeros([2, max_iter])\n",
    "\n",
    "    x = np.ones([num_samples, len(params)])\n",
    "    for i in range(num_samples):\n",
    "        x[i][1] = ip[i]\n",
    "    \n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    while iteration < max_iter:\n",
    "        cost[iteration] = compute_cost(ip, op, params)\n",
    "        params_store[:, iteration] = params\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {iteration}')\n",
    "        print(f'cost: {cost[iteration]}')\n",
    "\n",
    "\n",
    "        \n",
    "        # Apply batch gradient descent\n",
    "        for i in range(0, num_samples):\n",
    "            for j in range(len(params)):\n",
    "                prevParams = params\n",
    "                params[j] += (alpha / num_samples) * (op[i] - np.dot(prevParams, x[i])) * x[i][j]\n",
    "        iteration += 1\n",
    "    \n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qbjhyZ71kq_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13402684.89272019\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 25647.552486176028\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 11494.891882920292\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 11413.268490687566\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 11409.950445088098\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 11408.663329178691\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 11407.437035803543\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 11406.213137119677\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 11404.989892323218\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 11403.767249265296\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 11402.545206107552\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 11401.32376251068\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 11400.102918180128\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 11398.88267282289\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 11397.663026146092\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 11396.443977857045\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 11395.225527663137\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 11394.007675271965\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 11392.790420391239\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 11391.573762728822\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 11390.357701992696\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 11389.142237891025\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 11387.927370132067\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 11386.713098424289\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 11385.499422476214\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 11384.286341996583\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 11383.07385669424\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 11381.861966278191\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 11380.650670457566\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 11379.439968941637\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 11378.229861439842\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 11377.020347661728\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 11375.811427317032\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 11374.603100115559\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 11373.395365767314\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 11372.18822398244\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 11370.9816744712\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 11369.775716943994\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 11368.570351111393\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 11367.365576684091\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 11366.161393372928\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 11364.957800888862\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 11363.754798943031\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 11362.552387246711\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 11361.350565511257\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 11360.149333448247\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 11358.948690769363\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 11357.748637186403\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 11356.549172411338\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 11355.350296156299\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 11354.1520081335\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 11352.954308055367\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 11351.757195634378\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 11350.560670583227\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 11349.364732614718\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 11348.169381441792\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 11346.97461677754\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 11345.78043833519\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 11344.586845828104\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 11343.393838969794\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 11342.2014174739\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 11341.009581054215\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 11339.818329424666\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 11338.6276622993\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 11337.437579392368\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 11336.248080418178\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 11335.059165091214\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 11333.870833126104\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 11332.683084237622\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 11331.495918140667\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 11330.309334550266\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 11329.123333181602\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 11327.937913750007\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 11326.753075970955\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 11325.568819560005\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 11324.385144232907\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 11323.202049705551\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 11322.019535693924\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 11320.837601914207\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 11319.65624808268\n",
      "--------------------------\n",
      "iteration: 80\n",
      "cost: 11318.475473915765\n",
      "--------------------------\n",
      "iteration: 81\n",
      "cost: 11317.295279130036\n",
      "--------------------------\n",
      "iteration: 82\n",
      "cost: 11316.115663442186\n",
      "--------------------------\n",
      "iteration: 83\n",
      "cost: 11314.93662656908\n",
      "--------------------------\n",
      "iteration: 84\n",
      "cost: 11313.758168227696\n",
      "--------------------------\n",
      "iteration: 85\n",
      "cost: 11312.58028813515\n",
      "--------------------------\n",
      "iteration: 86\n",
      "cost: 11311.402986008687\n",
      "--------------------------\n",
      "iteration: 87\n",
      "cost: 11310.226261565713\n",
      "--------------------------\n",
      "iteration: 88\n",
      "cost: 11309.050114523781\n",
      "--------------------------\n",
      "iteration: 89\n",
      "cost: 11307.874544600541\n",
      "--------------------------\n",
      "iteration: 90\n",
      "cost: 11306.69955151382\n",
      "--------------------------\n",
      "iteration: 91\n",
      "cost: 11305.52513498156\n",
      "--------------------------\n",
      "iteration: 92\n",
      "cost: 11304.351294721837\n",
      "--------------------------\n",
      "iteration: 93\n",
      "cost: 11303.178030452878\n",
      "--------------------------\n",
      "iteration: 94\n",
      "cost: 11302.005341893058\n",
      "--------------------------\n",
      "iteration: 95\n",
      "cost: 11300.833228760866\n",
      "--------------------------\n",
      "iteration: 96\n",
      "cost: 11299.661690774916\n",
      "--------------------------\n",
      "iteration: 97\n",
      "cost: 11298.490727654018\n",
      "--------------------------\n",
      "iteration: 98\n",
      "cost: 11297.32033911708\n",
      "--------------------------\n",
      "iteration: 99\n",
      "cost: 11296.150524883096\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "# Training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
    "\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "\n",
    "alpha_batch = 1e-3\n",
    "max_iter = 100\n",
    "params_hat_batch, cost_batch, params_store_batch =\\\n",
    "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Implement Stochastic Gradient Descent from scratch. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEIJL-WGkq_I"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "Algorithm can be given as follows:\n",
    "```shuffle(x, y)\n",
    "for i in 0 -> m:\n",
    "    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Gx9LN0wQkq_I"
   },
   "outputs": [],
   "source": [
    "# I changed input_var, output_var to ip, op as it was not taking the parameter values but taking the input_var and output_var (the whole data) defined in the earlier block.\n",
    "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using stochastic gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(num_samples)\n",
    "    params_store = np.zeros([2, num_samples])\n",
    "\n",
    "    i = 0\n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    for x, y in zip(ip, op):\n",
    "        cost[i] = compute_cost(ip, op, params)\n",
    "        params_store[:, i] = params\n",
    "\n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {i}')\n",
    "        print(f'cost: {cost[i]}')\n",
    "\n",
    "        # Apply stochastic gradient descent\n",
    "        X = [1.0, x]\n",
    "        for j in range(len(params)):\n",
    "            prevParams = params\n",
    "            params[j] += (alpha / num_samples) * (y - np.dot(prevParams, X)) * X[j]\n",
    "        i+=1\n",
    "\n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HivE1gVkkq_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13402684.89272019\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 12285020.778753806\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 12123426.71300222\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 10551039.695068264\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 9750377.389334992\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 9747900.959248804\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 8007865.981455637\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 7980064.2423681915\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 6845169.750482736\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 5252867.329937665\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 5099129.674131528\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 3882560.0659284727\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 3445455.9905362627\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 3092677.9816934867\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 3031449.1229166174\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 3016725.510802723\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 2552723.282056797\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 2407240.485694568\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 2040519.9886308392\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 1743214.8644859127\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 1604547.4115329913\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 1251216.126475357\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 1054324.3896039606\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 1018327.9812998973\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 886514.8213430056\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 844897.3290680163\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 841250.3735667625\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 841254.585055187\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 737149.1793868678\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 610560.6280192692\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 586349.8316088499\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 460689.2027665352\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 421872.1793370531\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 413959.67932225636\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 391527.4125667411\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 316207.16596849693\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 308882.4747540845\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 300705.9051658138\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 301792.9309120938\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 301782.708244196\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 301963.3943239133\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 289497.0698894443\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 278139.3002549684\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 214872.97332472182\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 198595.21855197372\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 195110.2284315293\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 195059.7617665074\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 192509.3700040086\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 193392.54450830753\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 189718.88540668023\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 183572.94217365788\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 182303.64874344278\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 155584.85364366043\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 154232.47500913162\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 139296.34773502877\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 139777.65134297387\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 119592.87552162018\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 118137.29739789807\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 105087.22204605625\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 93316.26815352548\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 77415.59558700018\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 72992.66506919706\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 71744.09130050518\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 71932.30213955665\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 60520.29141317632\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 57755.86772514645\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 54970.29035508392\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 52536.84294646495\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 47937.47033047407\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 47880.12840473202\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 48608.646503462645\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 47248.47484367927\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 38488.4668428632\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 37839.06470945705\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 37834.30068791706\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 36567.90691344859\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 29814.688558732258\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 29032.90921699435\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 25721.035570557047\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 25350.89774939111\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "alpha = 1e-3\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "params_hat, cost, params_store =\\\n",
    "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error for Batch Gradient Descent: 107.75738547600952\n",
      "Root Mean Square Error for Stochastic Gradient Descent: 162.23990476697873\n"
     ]
    }
   ],
   "source": [
    "# Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm\n",
    "def calcRMSE(X, Y, params):\n",
    "    \"\"\"\n",
    "    Calculates the Root mean square error.\n",
    "    :param X: The input varaible of samples\n",
    "    :param Y: The actual output variable of samples.\n",
    "    :param params: The parameters of linears regression\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    Y_cap = np.zeros(len(Y))\n",
    "    for i in range(0, len(Y)):\n",
    "        Y_cap[i] = np.dot(params, [1.0, X[i]])\n",
    "    E = Y - Y_cap\n",
    "    return np.sqrt(np.sum(E*E)/len(E))\n",
    "\n",
    "print(\"Root Mean Square Error for Batch Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat_batch)))\n",
    "print(\"Root Mean Square Error for Stochastic Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1630902274461,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "930loAL6kq_L",
    "outputId": "e807576e-9852-4857-9a29-d367f2e0b26b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min cost with BGD: 11296.150524883096\n",
      "min cost with SGD: 25350.89774939111\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
    "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('normalized cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
    "print(f'min cost with SGD: {np.min(cost)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrpju6Kwkq_N"
   },
   "source": [
    "### Q2.4 Which linear regression model do you think works best for this data? Explain in brief. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbTux39kq_N"
   },
   "source": [
    "Based on how data is created, we have:\n",
    "\\begin{equation}\n",
    "y = 15 x + 2.4 + 300.0 * uniform(0, 1)\n",
    "\\end{equation}\n",
    "Thus, the model that works best for this data would be the one that provides the average results.\n",
    "The expected value of $uniform(0, 1)$ is $0.5$. Thus we have:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y &= 15 x + 2.4 + 300.0 * E(uniform(0, 1))\\\\\n",
    "y &= 15 x + 2.4 + 300 * (0.5)\\\\\n",
    "y &= 152.4 + 15x\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This will fit best with data and hence the best linear regression model.\n",
    "Comparing it with $y = \\theta_0 + \\theta_1 x$, we have $\\theta_0 = 152.4$ and $\\theta_1 = 15$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p02LYZrkq_N"
   },
   "source": [
    "### Q3. Linear Regression Analytical Problem (10 points)\n",
    "Consider the following training data.\n",
    "\n",
    "| X1 | X2 | Y |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1.5 |\n",
    "| 1 | 0 | 2 |\n",
    "| 1 | 1 | 2.5 |\n",
    "Suppose the data comes from a model y = $θ_{0}$ +$θ_{1}$x1 +$θ_{2}$x2 for unknown constants $θ_{0}$,$θ_{1}$,$θ_{2}$. Use least squares linear regression to find an estimate of $θ_{0}$,$θ_{1}$,$θ_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have:\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "        1 & 0 & 0\\\\\n",
    "        1 & 0 & 1\\\\\n",
    "        1 & 1 & 0\\\\\n",
    "        1 & 1 & 1\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Y = \\begin{bmatrix}\n",
    "        0\\\\\n",
    "        1.5\\\\\n",
    "        2\\\\\n",
    "        2.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "X^T = \\begin{bmatrix}\n",
    "        1 & 1 & 1 & 1\\\\\n",
    "        0 & 0 & 1 & 1\\\\\n",
    "        0 & 1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence,\n",
    "\\begin{equation}\n",
    "X^TX = \\begin{bmatrix}\n",
    "        4 & 2 & 2\\\\\n",
    "        2 & 2 & 1\\\\\n",
    "        2 & 1 & 2\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "|X^TX| = 4(4-1) -2(4-2)+2(2-4)=4\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Cf(X^TX) = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Adj(X^TX) = (Cf(X^TX))^T = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{|X^TX|} Adj(X^TX)= \\begin{bmatrix}\n",
    "        0.75 & -0.5 & -0.5\\\\\n",
    "        -0.5 & 1 & 0\\\\\n",
    "        -0.5 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1}X^T = \\begin{bmatrix}\n",
    "        0.75 & 0.25 & 0.25 & -0.25\\\\\n",
    "        -0.5 & -0.5 & 0.5 & 0.5\\\\\n",
    "        -0.5 & 0.5 & -0.5 & 0.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We have, $\\theta^* = (X^TX)^{-1}X^TY$\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "\\theta^* = \\begin{bmatrix}\n",
    "        \\theta_0\\\\\n",
    "        \\theta_1\\\\\n",
    "        \\theta_2\n",
    "    \\end{bmatrix}\n",
    "    =\\begin{bmatrix}\n",
    "        0.25\\\\\n",
    "        1.5\\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the linear regression equation becomes:\n",
    "\\begin{equation}\n",
    "y = 0.25 + 1.5X_1 + X_2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-1-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}