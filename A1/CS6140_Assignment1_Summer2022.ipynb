{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1avDQgkq-5"
   },
   "source": [
    "# CS 6140 Machine Learning: Assignment - 1 (Total Points: 100)\n",
    "## Prof. Ahmad Uzair "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFpvDelkq-6"
   },
   "source": [
    "### Q1. Decision Tree Classifier (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Growing Decison Trees from scratch (40 points)\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "You must also print the Decision Tree. Use information gain based on entropy as the splitting measure.\n",
    "\n",
    "Use the data.csv dataset for this particular question. The dataset should be uploaded on Canvas with Assignment 1. Split the dataset into training and test data and calculate testing accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utils class to perform certain calculations\n",
    "class Utils:\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)\n",
    "\n",
    "# Node of a DecisionTree. Can be either regular node or leaf node.\n",
    "# Normal node contains information about the feature and the value it compares in that node.\n",
    "# Leaf node contains the type of class.\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, X, Y):\n",
    "        if len(X) == 0:\n",
    "            return\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.isLeaf = False\n",
    "        self.classType = -1\n",
    "        self.H = self.entropy(Y)\n",
    "        self.trueChild = None\n",
    "        self.falseChild = None\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        for i in range(len(freq)):\n",
    "            if freq[i] == len(Y):\n",
    "                self.isLeaf = True\n",
    "                self.classType = num[i]\n",
    "                return\n",
    "\n",
    "        self.featureIndex, self.compValue = self.findBestSplit()\n",
    "        tx, ty, fx, fy = self.split(X, Y, self.featureIndex, self.compValue)\n",
    "        self.trueChild = DecisionTreeNode(tx, ty)\n",
    "        self.falseChild = DecisionTreeNode(fx, fy)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        h = 0.0\n",
    "        for val in freq:\n",
    "            if val != 0:\n",
    "                prob = val / len(Y)\n",
    "                h -= prob * (np.log2(prob))\n",
    "        return h\n",
    "\n",
    "    def informationGain(self, X, Y, featureIndex, value):\n",
    "        tx, ty, fx, fy = self.split(X, Y, featureIndex, value)\n",
    "        expectedEntropy = 0\n",
    "        expectedEntropy += (len(ty) / len(Y)) * self.entropy(ty)\n",
    "        expectedEntropy += (len(fy) / len(Y)) * self.entropy(fy)\n",
    "        IG = self.H - expectedEntropy\n",
    "        return IG\n",
    "\n",
    "    def split(self, X, Y, featureIndex, value):\n",
    "        tx, ty, fx, fy = [], [], [], []\n",
    "        for i in range(0, len(X)):\n",
    "            if X[i][featureIndex] < value:\n",
    "                tx.append(X[i])\n",
    "                ty.append(Y[i])\n",
    "            else:\n",
    "                fx.append(X[i])\n",
    "                fy.append(Y[i])\n",
    "        return np.array(tx), np.array(ty), np.array(fx), np.array(fy)\n",
    "\n",
    "    def findBestSplit(self):\n",
    "        copy_X = np.transpose(self.X)\n",
    "        maxIG = float(\"-inf\")\n",
    "        bestFeatureIndex = None\n",
    "        bestValue = None\n",
    "        for i in range(0, len(copy_X)):\n",
    "            T = np.sort(copy_X[i])\n",
    "            for j in range(1, len(T)):\n",
    "                midValue = (T[j - 1] + T[j]) / 2.0\n",
    "                currentIG = self.informationGain(self.X, self.Y, i, midValue)\n",
    "                if currentIG > maxIG:\n",
    "                    maxIG = currentIG\n",
    "                    bestFeatureIndex = i\n",
    "                    bestValue = midValue\n",
    "        return bestFeatureIndex, bestValue\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.isLeaf:\n",
    "            return self.classType\n",
    "        elif X[self.featureIndex] <= self.compValue:\n",
    "            return self.trueChild.predict(X)\n",
    "        else:\n",
    "            return self.falseChild.predict(X)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.isLeaf:\n",
    "            return \"class:\" + str(self.classType)\n",
    "        else:\n",
    "            return \"feature\" + str(self.featureIndex + 1) + \" <= \" + str(self.compValue)\n",
    "\n",
    "#The Decision Tree Class\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.root = DecisionTreeNode(X, Y)\n",
    "        return\n",
    "\n",
    "    def print(self, feature_names, class_names):\n",
    "        self.preOrder(self.root, feature_names, class_names, \"|--- \")\n",
    "\n",
    "    def preOrder(self, root, feature_names, class_names, prev):\n",
    "        if root == None:\n",
    "            return\n",
    "        if root.isLeaf:\n",
    "            print(prev + \"class: \" + class_names[root.classType])\n",
    "            return\n",
    "        print(prev + feature_names[root.featureIndex] + \" <= \" + str(root.compValue))\n",
    "        self.preOrder(root.trueChild, feature_names, class_names, \"|   \" + prev)\n",
    "        print(prev + feature_names[root.featureIndex] + \" >  \" + str(root.compValue))\n",
    "        self.preOrder(root.falseChild, feature_names, class_names, \"|   \" + prev)\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y = []\n",
    "        for i in range(len(X)):\n",
    "            Y.append(self.root.predict(X[i]))\n",
    "        return np.array(Y)\n",
    "\n",
    "    def accuracy(self, Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature4 <= 1.65\n",
      "|   |   |--- feature3 <= 5.0\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  5.0\n",
      "|   |   |   |--- feature1 <= 6.05\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  6.05\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.65\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature2 <= 3.1\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature2 >  3.1\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "Testing Accuracy: 94.74\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_csv(\"data.csv\")\n",
    "X = data.drop(\"class\", axis=1).values\n",
    "Y = data[\"class\"].values\n",
    "\n",
    "feature_names = list(data.drop(\"class\", axis=1).columns)\n",
    "class_names = [str(i) for i in range(0, len(set(Y)))]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "dt.print(feature_names, class_names)\n",
    "\n",
    "Y_test_pred = dt.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q1.2 Decision Tree using Sklearn Library (10 points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Decision Tree Classifier from the Sklearn Library and use gini index as a splitting measure. Use the data.csv dataset.\n",
    "Calculate accuracy for this model. \n",
    "Print the Decision tree and compare the Decision Trees generated from your code and Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature4 <= 0.80\n",
      "|   |--- class: 0\n",
      "|--- feature4 >  0.80\n",
      "|   |--- feature4 <= 1.65\n",
      "|   |   |--- feature3 <= 5.00\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  5.00\n",
      "|   |   |   |--- feature1 <= 6.05\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  6.05\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.65\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature2 <= 3.10\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature2 >  3.10\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "\n",
      "Testing Accuracy: 94.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(export_text(clf, feature_names=feature_names))\n",
    "\n",
    "Y_test_pred = clf.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparision of the two generated decision trees:\n",
    "\n",
    "Both the trees generated are almost identical most of the time.\n",
    "The difference in the trees could occur because of the different splitting measure (Information Gain for my code and Gini for sklearn code). The difference is still minimal and the accuracy achieved by both of the trees is similar.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEBH56pukq_H"
   },
   "source": [
    "### Q2 Linear Regression (40 points)\n",
    "\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FL1tuQEkq_H"
   },
   "source": [
    "## Gradient descent algorithm \n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This minimizes the following cost function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "h(x_i) = \\theta^T \\bar{x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "cOem1EjQkq_H"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "true_slope = 15\n",
    "true_intercept = 2.4\n",
    "input_var = np.arange(0.0,100.0)\n",
    "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1630902228487,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "SNvDqYEykq_H",
    "outputId": "c7f53823-73d9-473f-9e28-944f1f09a415",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.scatter(input_var, output_var)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XgNhbpEmkq_I"
   },
   "outputs": [],
   "source": [
    "def compute_cost(ip, op, params):\n",
    "    \"\"\"\n",
    "    Cost function in linear regression where the cost is calculated\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    num_samples = len(ip)\n",
    "    cost_sum = 0.0\n",
    "    for x,y in zip(ip, op):\n",
    "        y_hat = np.dot(params, np.array([1.0, x]))\n",
    "        cost_sum += (y_hat - y) ** 2\n",
    "    \n",
    "    cost = cost_sum / (num_samples)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Implement Linear Regression using Batch Gradient Descent from scratch.  (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ao7aYu9kq_I"
   },
   "source": [
    "\n",
    "### Batch gradient descent\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "f0z876gDkq_I"
   },
   "outputs": [],
   "source": [
    "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(max_iter)\n",
    "    params_store = np.zeros([2, max_iter])\n",
    "\n",
    "    X_Bar = [1.0, np.mean(ip)]\n",
    "    \n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    while iteration < max_iter:\n",
    "        cost[iteration] = compute_cost(ip, op, params)\n",
    "        params_store[:, iteration] = params\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {iteration}')\n",
    "        print(f'cost: {cost[iteration]}')\n",
    "        \n",
    "        \n",
    "        # Apply batch gradient descent\n",
    "        for i in range(0, num_samples):\n",
    "            for j in range(len(params)):\n",
    "                params[j] += (alpha / num_samples) * (op[i] - np.dot(params, [1.0, ip[i]])) * X_Bar[j]\n",
    "        iteration += 1\n",
    "    \n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "qbjhyZ71kq_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13071028.58237319\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 137351.23212364878\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 16801.981541950092\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 14362.63057628714\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 14211.19994552739\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 14198.152295045438\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 14196.985597547657\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 14196.880917089191\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 14196.871521885001\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 14196.870678630225\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 14196.870602944797\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 14196.870596151735\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 14196.870595542021\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 14196.870595487295\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 14196.870595482387\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 14196.870595481927\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 14196.870595481892\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 14196.870595481896\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 14196.870595481896\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 14196.870595481896\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 14196.870595481896\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 14196.870595481892\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 14196.870595481892\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 14196.870595481892\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 14196.870595481896\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 14196.870595481896\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 14196.870595481907\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 14196.870595481907\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 14196.870595481907\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 14196.870595481907\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 14196.87059548191\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 14196.87059548191\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 14196.87059548191\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 14196.87059548191\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 14196.870595481896\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 14196.870595481898\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 14196.870595481902\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 14196.870595481905\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 80\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 81\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 82\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 83\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 84\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 85\n",
      "cost: 14196.870595481925\n",
      "--------------------------\n",
      "iteration: 86\n",
      "cost: 14196.870595481925\n",
      "--------------------------\n",
      "iteration: 87\n",
      "cost: 14196.870595481934\n",
      "--------------------------\n",
      "iteration: 88\n",
      "cost: 14196.870595481936\n",
      "--------------------------\n",
      "iteration: 89\n",
      "cost: 14196.870595481936\n",
      "--------------------------\n",
      "iteration: 90\n",
      "cost: 14196.870595481936\n",
      "--------------------------\n",
      "iteration: 91\n",
      "cost: 14196.870595481936\n",
      "--------------------------\n",
      "iteration: 92\n",
      "cost: 14196.870595481936\n",
      "--------------------------\n",
      "iteration: 93\n",
      "cost: 14196.870595481922\n",
      "--------------------------\n",
      "iteration: 94\n",
      "cost: 14196.870595481925\n",
      "--------------------------\n",
      "iteration: 95\n",
      "cost: 14196.870595481925\n",
      "--------------------------\n",
      "iteration: 96\n",
      "cost: 14196.870595481925\n",
      "--------------------------\n",
      "iteration: 97\n",
      "cost: 14196.870595481925\n",
      "--------------------------\n",
      "iteration: 98\n",
      "cost: 14196.870595481927\n",
      "--------------------------\n",
      "iteration: 99\n",
      "cost: 14196.870595481927\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "# Training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
    "\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "\n",
    "alpha_batch = 1e-3\n",
    "max_iter = 100\n",
    "params_hat_batch, cost_batch, params_store_batch =\\\n",
    "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Implement Stochastic Gradient Descent from scratch. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEIJL-WGkq_I"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "Algorithm can be given as follows:\n",
    "```shuffle(x, y)\n",
    "for i in 0 -> m:\n",
    "    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Gx9LN0wQkq_I"
   },
   "outputs": [],
   "source": [
    "# I changed input_var, output_var to ip, op as it was not taking the parameter values but taking the input_var and output_var (the whole data) defined in the earlier block.\n",
    "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using stochastic gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(num_samples)\n",
    "    params_store = np.zeros([2, num_samples])\n",
    "\n",
    "    X_Bar = [1.0, np.mean(ip)]\n",
    "    print(X_Bar)\n",
    "\n",
    "    i = 0\n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    for x, y in zip(ip, op):\n",
    "        cost[i] = compute_cost(ip, op, params)\n",
    "        params_store[:, i] = params\n",
    "\n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {i}')\n",
    "        print(f'cost: {cost[i]}')\n",
    "\n",
    "        # Apply stochastic gradient descent\n",
    "        for j in range(len(params)):\n",
    "            params[j] += (alpha / num_samples) * (y - np.dot(params, [1.0, x])) * X_Bar[j]\n",
    "        i+=1\n",
    "\n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "HivE1gVkkq_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 48.575]\n",
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13071028.58237319\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 11856017.462897662\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 11691153.532945227\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 10823626.378920127\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 9615540.271435685\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 9575164.846824031\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 9302406.2344944\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 9168483.243194208\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 8730545.780131368\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 8310271.948010731\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 7633527.273374314\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 7387006.303600539\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 7160609.443446671\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 7039958.899216315\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 6863393.98683151\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 6564126.113276124\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 6218864.81455834\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 6210236.405086308\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 6043022.762305567\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 5440632.614003582\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 4947839.116694377\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 4558230.377906513\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 4188024.291494859\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 3754480.072559993\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 3511474.945244881\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 3346720.578459833\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 2947547.319613063\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 2882523.8537408314\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 2901901.412314189\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 2657604.416427019\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 2486371.471568593\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 2344762.8478782903\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 2097484.334760551\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 1898418.4977115947\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 1689467.4301522027\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 1619309.5586283836\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 1501614.453137707\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 1430035.5995203727\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 1432509.2283835933\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 1318604.175523176\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 1180241.1171969422\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 1075266.142147929\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 1010912.5805986645\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 923631.1302274093\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 913597.0484923145\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 911116.6859516263\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 915756.9902251388\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 845751.2751484308\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 818804.1494336699\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 832693.2052392906\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 843078.0195140414\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 750747.3555472904\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 677501.3227481793\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 620760.93545935\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 618122.7592526071\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 610657.9477163358\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 559010.2042142593\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 538206.9895124205\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 482213.8828325098\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 461299.8012508461\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 410319.79873901303\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 399812.9401723734\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 403695.13401744864\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 395969.7398507638\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 387556.8950935417\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 365985.63829597185\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 357076.9395603282\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 321036.07868768607\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 288228.63042703574\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 289537.36556378985\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 254136.58391222934\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 225858.7836805458\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 204577.46486233146\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 200945.33003268152\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 180000.06355085436\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 158727.55443262644\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 159876.07023631607\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 145734.29567990662\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 145757.4685715731\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 138844.3886919023\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "alpha = 1e-3\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "params_hat, cost, params_store =\\\n",
    "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error for Batch Gradient Descent: 82.6801929514101\n",
      "Root Mean Square Error for Stochastic Gradient Descent: 346.531955495187\n"
     ]
    }
   ],
   "source": [
    "# Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm\n",
    "def calcRMSE(X, Y, params):\n",
    "    Y_cap = np.zeros(len(Y))\n",
    "    for i in range(0, len(Y)):\n",
    "        Y_cap[i] = np.dot(params, [1.0, X[i]])\n",
    "    E = Y - Y_cap\n",
    "    return np.sqrt(np.sum(E*E)/len(E))\n",
    "\n",
    "print(\"Root Mean Square Error for Batch Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat_batch)))\n",
    "print(\"Root Mean Square Error for Stochastic Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1630902274461,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "930loAL6kq_L",
    "outputId": "e807576e-9852-4857-9a29-d367f2e0b26b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min cost with BGD: 14196.870595481892\n",
      "min cost with SGD: 138844.3886919023\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
    "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('normalized cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
    "print(f'min cost with SGD: {np.min(cost)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrpju6Kwkq_N"
   },
   "source": [
    "### Q2.4 Which linear regression model do you think works best for this data? Explain in brief. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbTux39kq_N"
   },
   "source": [
    "Based on how data is created, we have:\n",
    "\\begin{equation}\n",
    "y = 15 x + 2.4 + 300.0 * uniform(0, 1)\n",
    "\\end{equation}\n",
    "Thus, the model that works best for this data would be the one that provides the average results.\n",
    "The expected value of $uniform(0, 1)$ is $0.5$. Thus we have:\n",
    "\\begin{equation}\n",
    "y &= 15 x + 2.4 + 300.0 * E(uniform(0, 1))\\\\\n",
    "y &= 15 x + 2.4 + 300 * (0.5)\\\\\n",
    "y &= 152.4 + 15x\n",
    "\\end{equation}\n",
    "\n",
    "This will fit best with data and hence the best linear regression model.\n",
    "Comparing it with $y = \\theta_0 + \\theta_1 x$, we have $\\theta_0 = 152.4$ and $\\theta_1 = 15$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p02LYZrkq_N"
   },
   "source": [
    "### Q3. Linear Regression Analytical Problem (10 points)\n",
    "Consider the following training data.\n",
    "\n",
    "| X1 | X2 | Y |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1.5 |\n",
    "| 1 | 0 | 2 |\n",
    "| 1 | 1 | 2.5 |\n",
    "Suppose the data comes from a model y = $_{0}$ +$_{1}$x1 +$_{2}$x2 for unknown constants $_{0}$,$_{1}$,$_{2}$. Use least squares linear regression to find an estimate of $_{0}$,$_{1}$,$_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have:\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "        1 & 0 & 0\\\\\n",
    "        1 & 0 & 1\\\\\n",
    "        1 & 1 & 0\\\\\n",
    "        1 & 1 & 1\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Y = \\begin{bmatrix}\n",
    "        0\\\\\n",
    "        1.5\\\\\n",
    "        2\\\\\n",
    "        2.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "X^T = \\begin{bmatrix}\n",
    "        1 & 1 & 1 & 1\\\\\n",
    "        0 & 0 & 1 & 1\\\\\n",
    "        0 & 1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence,\n",
    "\\begin{equation}\n",
    "X^TX = \\begin{bmatrix}\n",
    "        4 & 2 & 2\\\\\n",
    "        2 & 2 & 1\\\\\n",
    "        2 & 1 & 2\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "|X^TX| &= 4(4-1) -2(4-2)+2(2-4)\\\\\n",
    "        &=4\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Cf(X^TX) = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Adj(X^TX) = (Cf(X^TX))^T = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} &= \\frac{1}{|X^TX|} Adj(X^TX)\\\\\n",
    "&= \\begin{bmatrix}\n",
    "        0.75 & -0.5 & -0.5\\\\\n",
    "        -0.5 & 1 & 0\\\\\n",
    "        -0.5 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1}X^T = \\begin{bmatrix}\n",
    "        0.75 & 0.25 & 0.25 & -0.25\\\\\n",
    "        -0.5 & -0.5 & 0.5 & 0.5\\\\\n",
    "        -0.5 & 0.5 & -0.5 & 0.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We have, $\\theta^* = (X^TX)^{-1}X^TY$\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "\\theta^* = \\begin{bmatrix}\n",
    "        \\theta_0\\\\\n",
    "        \\theta_1\\\\\n",
    "        \\theta_2\n",
    "    \\end{bmatrix}\n",
    "    =\\begin{bmatrix}\n",
    "        0.25\\\\\n",
    "        1.5\\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the linear regression equation becomes:\n",
    "\\begin{equation}\n",
    "y = 0.25 + 1.5X_1 + X_2\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-1-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}