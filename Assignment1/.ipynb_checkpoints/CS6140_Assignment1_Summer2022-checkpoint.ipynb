{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1avDQgkq-5"
   },
   "source": [
    "# CS 6140 Machine Learning: Assignment - 1 (Total Points: 100)\n",
    "## Prof. Ahmad Uzair "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFpvDelkq-6"
   },
   "source": [
    "### Q1. Decision Tree Classifier (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Growing Decison Trees from scratch (40 points)\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "You must also print the Decision Tree. Use information gain based on entropy as the splitting measure.\n",
    "\n",
    "Use the data.csv dataset for this particular question. The dataset should be uploaded on Canvas with Assignment 1. Split the dataset into training and test data and calculate testing accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utils class to perform certain calculations\n",
    "class Utils:\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)\n",
    "\n",
    "# Node of a DecisionTree. Can be either regular node or leaf node.\n",
    "# Normal node contains information about the feature and the value it compares in that node.\n",
    "# Leaf node contains the type of class.\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, X, Y):\n",
    "        if len(X) == 0:\n",
    "            return\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.isLeaf = False\n",
    "        self.classType = -1\n",
    "        self.H = self.entropy(Y)\n",
    "        self.trueChild = None\n",
    "        self.falseChild = None\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        for i in range(len(freq)):\n",
    "            if freq[i] == len(Y):\n",
    "                self.isLeaf = True\n",
    "                self.classType = num[i]\n",
    "                return\n",
    "\n",
    "        self.featureIndex, self.compValue = self.findBestSplit()\n",
    "        tx, ty, fx, fy = self.split(X, Y, self.featureIndex, self.compValue)\n",
    "        self.trueChild = DecisionTreeNode(tx, ty)\n",
    "        self.falseChild = DecisionTreeNode(fx, fy)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        h = 0.0\n",
    "        for val in freq:\n",
    "            if val != 0:\n",
    "                prob = val / len(Y)\n",
    "                h -= prob * (np.log2(prob))\n",
    "        return h\n",
    "\n",
    "    def informationGain(self, X, Y, featureIndex, value):\n",
    "        tx, ty, fx, fy = self.split(X, Y, featureIndex, value)\n",
    "        expectedEntropy = 0\n",
    "        expectedEntropy += (len(ty) / len(Y)) * self.entropy(ty)\n",
    "        expectedEntropy += (len(fy) / len(Y)) * self.entropy(fy)\n",
    "        IG = self.H - expectedEntropy\n",
    "        return IG\n",
    "\n",
    "    def split(self, X, Y, featureIndex, value):\n",
    "        tx, ty, fx, fy = [], [], [], []\n",
    "        for i in range(0, len(X)):\n",
    "            if X[i][featureIndex] < value:\n",
    "                tx.append(X[i])\n",
    "                ty.append(Y[i])\n",
    "            else:\n",
    "                fx.append(X[i])\n",
    "                fy.append(Y[i])\n",
    "        return np.array(tx), np.array(ty), np.array(fx), np.array(fy)\n",
    "\n",
    "    def findBestSplit(self):\n",
    "        copy_X = np.transpose(self.X)\n",
    "        maxIG = float(\"-inf\")\n",
    "        bestFeatureIndex = None\n",
    "        bestValue = None\n",
    "        for i in range(0, len(copy_X)):\n",
    "            T = np.sort(copy_X[i])\n",
    "            for j in range(1, len(T)):\n",
    "                midValue = (T[j - 1] + T[j]) / 2.0\n",
    "                currentIG = self.informationGain(self.X, self.Y, i, midValue)\n",
    "                if currentIG > maxIG:\n",
    "                    maxIG = currentIG\n",
    "                    bestFeatureIndex = i\n",
    "                    bestValue = midValue\n",
    "        return bestFeatureIndex, bestValue\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.isLeaf:\n",
    "            return self.classType\n",
    "        elif X[self.featureIndex] <= self.compValue:\n",
    "            return self.trueChild.predict(X)\n",
    "        else:\n",
    "            return self.falseChild.predict(X)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.isLeaf:\n",
    "            return \"class:\" + str(self.classType)\n",
    "        else:\n",
    "            return \"feature\" + str(self.featureIndex + 1) + \" <= \" + str(self.compValue)\n",
    "\n",
    "#The Decision Tree Class\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.root = DecisionTreeNode(X, Y)\n",
    "        return\n",
    "\n",
    "    def print(self, feature_names, class_names):\n",
    "        self.preOrder(self.root, feature_names, class_names, \"|--- \")\n",
    "\n",
    "    def preOrder(self, root, feature_names, class_names, prev):\n",
    "        if root == None:\n",
    "            return\n",
    "        if root.isLeaf:\n",
    "            print(prev + \"class: \" + class_names[root.classType])\n",
    "            return\n",
    "        print(prev + feature_names[root.featureIndex] + \" <= \" + str(root.compValue))\n",
    "        self.preOrder(root.trueChild, feature_names, class_names, \"|   \" + prev)\n",
    "        print(prev + feature_names[root.featureIndex] + \" >  \" + str(root.compValue))\n",
    "        self.preOrder(root.falseChild, feature_names, class_names, \"|   \" + prev)\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y = []\n",
    "        for i in range(len(X)):\n",
    "            Y.append(self.root.predict(X[i]))\n",
    "        return np.array(Y)\n",
    "\n",
    "    def accuracy(self, Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature4 <= 1.75\n",
      "|   |   |--- feature3 <= 4.95\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.95\n",
      "|   |   |   |--- feature4 <= 1.55\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature4 >  1.55\n",
      "|   |   |   |   |--- feature1 <= 6.95\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature1 >  6.95\n",
      "|   |   |   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.75\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature1 <= 5.95\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  5.95\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "Testing Accuracy: 97.37\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_csv(\"data.csv\")\n",
    "X = data.drop(\"class\", axis=1).values\n",
    "Y = data[\"class\"].values\n",
    "\n",
    "feature_names = list(data.drop(\"class\", axis=1).columns)\n",
    "class_names = [str(i) for i in range(0, len(set(Y)))]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "dt.print(feature_names, class_names)\n",
    "\n",
    "Y_test_pred = dt.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Decision Tree using Sklearn Library (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Decision Tree Classifier from the Sklearn Library and use gini index as a splitting measure. Use the data.csv dataset.\n",
    "Calculate accuracy for this model. \n",
    "Print the Decision tree and compare the Decision Trees generated from your code and Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature4 <= 1.75\n",
      "|   |   |--- feature3 <= 4.95\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.95\n",
      "|   |   |   |--- feature4 <= 1.55\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature4 >  1.55\n",
      "|   |   |   |   |--- feature1 <= 6.95\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature1 >  6.95\n",
      "|   |   |   |   |   |--- class: 2\n",
      "|   |--- feature4 >  1.75\n",
      "|   |   |--- feature3 <= 4.85\n",
      "|   |   |   |--- feature2 <= 3.10\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature2 >  3.10\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- feature3 >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "\n",
      "Testing Accuracy: 97.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(export_text(clf, feature_names=feature_names))\n",
    "\n",
    "Y_test_pred = clf.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Comparision of the two generated decision trees:\n",
    "\n",
    "Both the trees generated are almost identical most of the time.\n",
    "The difference in the trees could occur because of the different splitting measure (Information Gain for my code and Gini for sklearn code). The difference is still minimal and the accuracy achieved by both of the trees is similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEBH56pukq_H"
   },
   "source": [
    "### Q2 Linear Regression (40 points)\n",
    "\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FL1tuQEkq_H"
   },
   "source": [
    "## Gradient descent algorithm \n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This minimizes the following cost function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "h(x_i) = \\theta^T \\bar{x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cOem1EjQkq_H"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "true_slope = 15\n",
    "true_intercept = 2.4\n",
    "input_var = np.arange(0.0,100.0)\n",
    "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1630902228487,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "SNvDqYEykq_H",
    "outputId": "c7f53823-73d9-473f-9e28-944f1f09a415",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.scatter(input_var, output_var)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XgNhbpEmkq_I"
   },
   "outputs": [],
   "source": [
    "def compute_cost(ip, op, params):\n",
    "    \"\"\"\n",
    "    Cost function in linear regression where the cost is calculated\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    num_samples = len(ip)\n",
    "    cost_sum = 0.0\n",
    "    for x,y in zip(ip, op):\n",
    "        y_hat = np.dot(params, np.array([1.0, x]))\n",
    "        cost_sum += (y_hat - y) ** 2\n",
    "    \n",
    "    cost = cost_sum / (num_samples)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Implement Linear Regression using Batch Gradient Descent from scratch.  (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ao7aYu9kq_I"
   },
   "source": [
    "\n",
    "### Batch gradient descent\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f0z876gDkq_I"
   },
   "outputs": [],
   "source": [
    "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(max_iter)\n",
    "    params_store = np.zeros([2, max_iter])\n",
    "\n",
    "    X_Bar = [1.0, np.mean(ip)]\n",
    "    \n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    while iteration < max_iter:\n",
    "        cost[iteration] = compute_cost(ip, op, params)\n",
    "        params_store[:, iteration] = params\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {iteration}')\n",
    "        print(f'cost: {cost[iteration]}')\n",
    "        \n",
    "        \n",
    "        # Apply batch gradient descent\n",
    "        for i in range(0, num_samples):\n",
    "            for j in range(len(params)):\n",
    "                prevParams = params\n",
    "                params[j] += (alpha / num_samples) * (op[i] - np.dot(prevParams, [1.0, ip[i]])) * X_Bar[j]\n",
    "        iteration += 1\n",
    "    \n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qbjhyZ71kq_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13701952.02964991\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 113618.3061516091\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 18723.220141375186\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 16403.368432973344\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 16260.526283322068\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 16250.45212797935\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 16249.734094696902\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 16249.682878229858\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 16249.679224822114\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 16249.678964213725\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 16249.6789456238\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 16249.678944297688\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 16249.67894420315\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 16249.678944196388\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 16249.678944195879\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 16249.67894419581\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 16249.67894419579\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 16249.678944195786\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 16249.678944195784\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 16249.678944195784\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 16249.678944195784\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 16249.678944195784\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 16249.678944195784\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 16249.678944195784\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 16249.678944195784\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 16249.678944195784\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 16249.67894419578\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 16249.67894419578\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 16249.67894419578\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 16249.67894419578\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 16249.67894419578\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 16249.67894419578\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 16249.678944195777\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 16249.678944195799\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 16249.678944195799\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 16249.678944195799\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 16249.678944195799\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 16249.678944195799\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 16249.678944195799\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 16249.678944195799\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 16249.678944195799\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 16249.678944195795\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 16249.678944195795\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 16249.678944195795\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 16249.678944195795\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 16249.678944195795\n",
      "--------------------------\n",
      "iteration: 80\n",
      "cost: 16249.678944195795\n",
      "--------------------------\n",
      "iteration: 81\n",
      "cost: 16249.678944195795\n",
      "--------------------------\n",
      "iteration: 82\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 83\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 84\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 85\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 86\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 87\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 88\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 89\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 90\n",
      "cost: 16249.678944195792\n",
      "--------------------------\n",
      "iteration: 91\n",
      "cost: 16249.67894419577\n",
      "--------------------------\n",
      "iteration: 92\n",
      "cost: 16249.67894419577\n",
      "--------------------------\n",
      "iteration: 93\n",
      "cost: 16249.67894419577\n",
      "--------------------------\n",
      "iteration: 94\n",
      "cost: 16249.67894419577\n",
      "--------------------------\n",
      "iteration: 95\n",
      "cost: 16249.67894419577\n",
      "--------------------------\n",
      "iteration: 96\n",
      "cost: 16249.678944195766\n",
      "--------------------------\n",
      "iteration: 97\n",
      "cost: 16249.678944195766\n",
      "--------------------------\n",
      "iteration: 98\n",
      "cost: 16249.678944195766\n",
      "--------------------------\n",
      "iteration: 99\n",
      "cost: 16249.678944195766\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "# Training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
    "\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "\n",
    "alpha_batch = 1e-3\n",
    "max_iter = 100\n",
    "params_hat_batch, cost_batch, params_store_batch =\\\n",
    "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Implement Stochastic Gradient Descent from scratch. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEIJL-WGkq_I"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "Algorithm can be given as follows:\n",
    "```shuffle(x, y)\n",
    "for i in 0 -> m:\n",
    "    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Gx9LN0wQkq_I"
   },
   "outputs": [],
   "source": [
    "# I changed input_var, output_var to ip, op as it was not taking the parameter values but taking the input_var and output_var (the whole data) defined in the earlier block.\n",
    "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using stochastic gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(num_samples)\n",
    "    params_store = np.zeros([2, num_samples])\n",
    "\n",
    "    X_Bar = [1.0, np.mean(ip)]\n",
    "    print(X_Bar)\n",
    "\n",
    "    i = 0\n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    for x, y in zip(ip, op):\n",
    "        cost[i] = compute_cost(ip, op, params)\n",
    "        params_store[:, i] = params\n",
    "\n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {i}')\n",
    "        print(f'cost: {cost[i]}')\n",
    "\n",
    "        # Apply stochastic gradient descent\n",
    "        for j in range(len(params)):\n",
    "            prevParams = params\n",
    "            params[j] += (alpha / num_samples) * (y - np.dot(prevParams, [1.0, x])) * X_Bar[j]\n",
    "        i+=1\n",
    "\n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HivE1gVkkq_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 50.8125]\n",
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13701952.02964991\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 12052934.354402237\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 11097036.125446545\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 10707982.967918849\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 10347912.43417023\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 9887633.30471741\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 8860664.052627075\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 8033251.994721967\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 8001949.409430647\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 7303714.421053681\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 6715863.063180762\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 6100076.406810844\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 6005083.249187248\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 5289594.731711193\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 4907296.284473543\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 4879886.947947339\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 4372839.908805151\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 4067886.4218167784\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 4014201.430222442\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 3927221.0947931334\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 3712756.165617857\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 3274996.5832937523\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 2885816.309619543\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 2554241.9979166035\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 2302496.530506348\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 2245094.035827523\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 2267240.019275367\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 2239115.4166785674\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 2080592.146417759\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 1884492.6826713285\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 1716560.9741254293\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 1660676.4107643578\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 1591927.557574733\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 1442660.732038803\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 1278940.8579814802\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 1218130.4363007455\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 1185420.2833654145\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 1037281.6643097071\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 926121.808117214\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 908122.9587878933\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 873597.7011330798\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 869946.7607660089\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 811791.922317854\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 754255.519647604\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 719790.3161409835\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 678566.7879234653\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 606261.5055481852\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 616331.6983280153\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 625425.9516412\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 606810.3280083147\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 577353.9413101494\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 505574.4800591321\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 443147.714766014\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 440789.5540378634\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 421170.4806744067\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 380688.1685418508\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 335492.68369562615\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 330359.12684718583\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 298867.8226059844\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 294224.92007708224\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 275211.39882826747\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 268036.8862538572\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 255373.59723302763\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 238862.73947994568\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 220667.56343248472\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 193968.9210767971\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 178447.84081053495\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 162304.14354368838\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 169084.36900416683\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 168190.89302101446\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 158599.80979669088\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 159198.7572395127\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 157306.66281408144\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 150707.23988518066\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 137661.84851466445\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 137737.7000634576\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 137856.6526791832\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 130084.31223543477\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 123691.8584135815\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 119889.08743193408\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "alpha = 1e-3\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "params_hat, cost, params_store =\\\n",
    "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error for Batch Gradient Descent: 98.09371385520579\n",
      "Root Mean Square Error for Stochastic Gradient Descent: 287.5295557308198\n"
     ]
    }
   ],
   "source": [
    "# Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm\n",
    "def calcRMSE(X, Y, params):\n",
    "    Y_cap = np.zeros(len(Y))\n",
    "    for i in range(0, len(Y)):\n",
    "        Y_cap[i] = np.dot(params, [1.0, X[i]])\n",
    "    E = Y - Y_cap\n",
    "    return np.sqrt(np.sum(E*E)/len(E))\n",
    "\n",
    "print(\"Root Mean Square Error for Batch Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat_batch)))\n",
    "print(\"Root Mean Square Error for Stochastic Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1630902274461,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "930loAL6kq_L",
    "outputId": "e807576e-9852-4857-9a29-d367f2e0b26b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min cost with BGD: 16249.678944195766\n",
      "min cost with SGD: 119889.08743193408\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
    "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('normalized cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
    "print(f'min cost with SGD: {np.min(cost)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrpju6Kwkq_N"
   },
   "source": [
    "### Q2.4 Which linear regression model do you think works best for this data? Explain in brief. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbTux39kq_N"
   },
   "source": [
    "Based on how data is created, we have:\n",
    "\\begin{equation}\n",
    "y = 15 x + 2.4 + 300.0 * uniform(0, 1)\n",
    "\\end{equation}\n",
    "Thus, the model that works best for this data would be the one that provides the average results.\n",
    "The expected value of $uniform(0, 1)$ is $0.5$. Thus we have:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y &= 15 x + 2.4 + 300.0 * E(uniform(0, 1))\\\\\n",
    "y &= 15 x + 2.4 + 300 * (0.5)\\\\\n",
    "y &= 152.4 + 15x\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This will fit best with data and hence the best linear regression model.\n",
    "Comparing it with $y = \\theta_0 + \\theta_1 x$, we have $\\theta_0 = 152.4$ and $\\theta_1 = 15$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p02LYZrkq_N"
   },
   "source": [
    "### Q3. Linear Regression Analytical Problem (10 points)\n",
    "Consider the following training data.\n",
    "\n",
    "| X1 | X2 | Y |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1.5 |\n",
    "| 1 | 0 | 2 |\n",
    "| 1 | 1 | 2.5 |\n",
    "Suppose the data comes from a model y = $_{0}$ +$_{1}$x1 +$_{2}$x2 for unknown constants $_{0}$,$_{1}$,$_{2}$. Use least squares linear regression to find an estimate of $_{0}$,$_{1}$,$_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have:\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "        1 & 0 & 0\\\\\n",
    "        1 & 0 & 1\\\\\n",
    "        1 & 1 & 0\\\\\n",
    "        1 & 1 & 1\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Y = \\begin{bmatrix}\n",
    "        0\\\\\n",
    "        1.5\\\\\n",
    "        2\\\\\n",
    "        2.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "X^T = \\begin{bmatrix}\n",
    "        1 & 1 & 1 & 1\\\\\n",
    "        0 & 0 & 1 & 1\\\\\n",
    "        0 & 1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence,\n",
    "\\begin{equation}\n",
    "X^TX = \\begin{bmatrix}\n",
    "        4 & 2 & 2\\\\\n",
    "        2 & 2 & 1\\\\\n",
    "        2 & 1 & 2\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "|X^TX| = 4(4-1) -2(4-2)+2(2-4)=4\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Cf(X^TX) = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Adj(X^TX) = (Cf(X^TX))^T = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{|X^TX|} Adj(X^TX)= \\begin{bmatrix}\n",
    "        0.75 & -0.5 & -0.5\\\\\n",
    "        -0.5 & 1 & 0\\\\\n",
    "        -0.5 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1}X^T = \\begin{bmatrix}\n",
    "        0.75 & 0.25 & 0.25 & -0.25\\\\\n",
    "        -0.5 & -0.5 & 0.5 & 0.5\\\\\n",
    "        -0.5 & 0.5 & -0.5 & 0.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We have, $\\theta^* = (X^TX)^{-1}X^TY$\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "\\theta^* = \\begin{bmatrix}\n",
    "        \\theta_0\\\\\n",
    "        \\theta_1\\\\\n",
    "        \\theta_2\n",
    "    \\end{bmatrix}\n",
    "    =\\begin{bmatrix}\n",
    "        0.25\\\\\n",
    "        1.5\\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the linear regression equation becomes:\n",
    "\\begin{equation}\n",
    "y = 0.25 + 1.5X_1 + X_2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-1-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}