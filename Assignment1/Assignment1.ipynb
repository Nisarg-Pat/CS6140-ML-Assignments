{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH1avDQgkq-5"
   },
   "source": [
    "# CS 6140 Machine Learning: Assignment - 1 (Total Points: 100)\n",
    "## Prof. Ahmad Uzair "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFpvDelkq-6"
   },
   "source": [
    "### Q1. Decision Tree Classifier (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Growing Decison Trees from scratch (40 points)\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal of this question in the assignment is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "You must also print the Decision Tree. Use information gain based on entropy as the splitting measure.\n",
    "\n",
    "Use the data.csv dataset for this particular question. The dataset should be uploaded on Canvas with Assignment 1. Split the dataset into training and test data and calculate testing accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Utils class to perform certain calculations\n",
    "class Utils:\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(Y, Y_hat):\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)\n",
    "\n",
    "\n",
    "# Node of a DecisionTree. Can be either regular node or leaf node.\n",
    "# Normal node contains information about the feature and the value it compares in that node.\n",
    "# Leaf node contains the type of class.\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, X, Y):\n",
    "        if len(X) == 0:\n",
    "            return\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.isLeaf = False\n",
    "        self.classType = -1\n",
    "        self.H = self.entropy(Y)\n",
    "        self.trueChild = None\n",
    "        self.falseChild = None\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        for i in range(len(freq)):\n",
    "            if freq[i] == len(Y):\n",
    "                self.isLeaf = True\n",
    "                self.classType = num[i]\n",
    "                return\n",
    "\n",
    "        self.featureIndex, self.compValue = self.findBestSplit()\n",
    "        tx, ty, fx, fy = self.split(X, Y, self.featureIndex, self.compValue)\n",
    "        self.trueChild = DecisionTreeNode(tx, ty)\n",
    "        self.falseChild = DecisionTreeNode(fx, fy)\n",
    "\n",
    "    def entropy(self, Y):\n",
    "        \"\"\"\n",
    "        Calculates Entropy of an array.\n",
    "        :param Y: Array\n",
    "        :return: Entropy of the array\n",
    "        \"\"\"\n",
    "\n",
    "        num, freq = np.unique(Y, return_counts=True)\n",
    "        h = 0.0\n",
    "        for val in freq:\n",
    "            if val != 0:\n",
    "                prob = val / len(Y)\n",
    "                h -= prob * (np.log2(prob))\n",
    "        return h\n",
    "\n",
    "    def informationGain(self, X, Y, featureIndex, value):\n",
    "        \"\"\"\n",
    "        Calculates Information Gain based on split featureIndex <= value\n",
    "        :param X: feature values of samples\n",
    "        :param Y: class of samples\n",
    "        :param featureIndex: the index of feature on which to split\n",
    "        :param value: the value to compare X[featureIndex] with\n",
    "        :return: Information Gain by the split\n",
    "        \"\"\"\n",
    "\n",
    "        tx, ty, fx, fy = self.split(X, Y, featureIndex, value)\n",
    "        expectedEntropy = 0\n",
    "        expectedEntropy += (len(ty) / len(Y)) * self.entropy(ty)\n",
    "        expectedEntropy += (len(fy) / len(Y)) * self.entropy(fy)\n",
    "        IG = self.H - expectedEntropy\n",
    "        return IG\n",
    "\n",
    "    def split(self, X, Y, featureIndex, value):\n",
    "        \"\"\"\n",
    "        Splits X and Y based on X[featureIndex] <= value into two arrays.\n",
    "        :param X: feature values of samples\n",
    "        :param Y: class of samples\n",
    "        :param featureIndex: the index of feature on which to split\n",
    "        :param value: the value to compare X[featureIndex] with\n",
    "        :return: the split of X and Y based on the condition\n",
    "        \"\"\"\n",
    "\n",
    "        tx, ty, fx, fy = [], [], [], []\n",
    "        for i in range(0, len(X)):\n",
    "            if X[i][featureIndex] < value:\n",
    "                tx.append(X[i])\n",
    "                ty.append(Y[i])\n",
    "            else:\n",
    "                fx.append(X[i])\n",
    "                fy.append(Y[i])\n",
    "        return np.array(tx), np.array(ty), np.array(fx), np.array(fy)\n",
    "\n",
    "    def findBestSplit(self):\n",
    "        \"\"\"\n",
    "        Finds the best split based on Information Gain.\n",
    "        Values are selected as midpoints of adjecent feature values in sorted order\n",
    "        :return: The featureIndex and the value which gives the best split\n",
    "        \"\"\"\n",
    "        copy_X = np.transpose(self.X)\n",
    "        maxIG = float(\"-inf\")\n",
    "        bestFeatureIndex = None\n",
    "        bestValue = None\n",
    "        for i in range(0, len(copy_X)):\n",
    "            T = np.sort(copy_X[i])\n",
    "            for j in range(1, len(T)):\n",
    "                midValue = (T[j - 1] + T[j]) / 2.0\n",
    "                currentIG = self.informationGain(self.X, self.Y, i, midValue)\n",
    "                if currentIG > maxIG:\n",
    "                    maxIG = currentIG\n",
    "                    bestFeatureIndex = i\n",
    "                    bestValue = midValue\n",
    "        return bestFeatureIndex, bestValue\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class based on the generated decision nodes\n",
    "        :param X: features of a sample\n",
    "        :return: the predicted class of the sample\n",
    "        \"\"\"\n",
    "\n",
    "        if self.isLeaf:\n",
    "            return self.classType\n",
    "        elif X[self.featureIndex] <= self.compValue:\n",
    "            return self.trueChild.predict(X)\n",
    "        else:\n",
    "            return self.falseChild.predict(X)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        String representation of the node\n",
    "        :return: String representation of the node\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isLeaf:\n",
    "            return \"class:\" + str(self.classType)\n",
    "        else:\n",
    "            return \"feature\" + str(self.featureIndex + 1) + \" <= \" + str(self.compValue)\n",
    "\n",
    "\n",
    "#The Decision Tree Class\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Learns the decision tree based on the given features and classes\n",
    "        :param X: feature values of a sample\n",
    "        :param Y: classes of the samples\n",
    "        \"\"\"\n",
    "        self.root = DecisionTreeNode(X, Y)\n",
    "\n",
    "    def print(self, feature_names, class_names):\n",
    "        \"\"\"\n",
    "        Prints the decision tree as in sklearn's export_text.\n",
    "        :param feature_names: names of the features\n",
    "        :param class_names: Names of the classes\n",
    "        \"\"\"\n",
    "        self.preOrder(self.root, feature_names, class_names, \"|--- \")\n",
    "\n",
    "    def preOrder(self, root, feature_names, class_names, prev):\n",
    "        \"\"\"\n",
    "        Helper function to print.\n",
    "        \"\"\"\n",
    "        if root == None:\n",
    "            return\n",
    "        if root.isLeaf:\n",
    "            print(prev + \"class: \" + class_names[root.classType])\n",
    "            return\n",
    "        print(prev + feature_names[root.featureIndex] + \" <= \" + str(root.compValue))\n",
    "        self.preOrder(root.trueChild, feature_names, class_names, \"|   \" + prev)\n",
    "        print(prev + feature_names[root.featureIndex] + \" >  \" + str(root.compValue))\n",
    "        self.preOrder(root.falseChild, feature_names, class_names, \"|   \" + prev)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the classes of samples\n",
    "        :param X: features of samples\n",
    "        :return: the predicted class of every samples\n",
    "        \"\"\"\n",
    "        Y = []\n",
    "        for i in range(len(X)):\n",
    "            Y.append(self.root.predict(X[i]))\n",
    "        return np.array(Y)\n",
    "\n",
    "    def accuracy(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Returns the accuracy of the decision tree\n",
    "        :param Y: The actual class of samples\n",
    "        :param Y_hat: The predicted class of samples\n",
    "        :return: The accuracy of the decision tree\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for i in range(len(Y)):\n",
    "            if Y[i] == Y_hat[i]:\n",
    "                count += 1\n",
    "        return count / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature3 <= 4.95\n",
      "|   |   |--- feature4 <= 1.7000000000000002\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature4 >  1.7000000000000002\n",
      "|   |   |   |--- feature2 <= 3.1\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature2 >  3.1\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |--- feature3 >  4.95\n",
      "|   |   |--- feature4 <= 1.7000000000000002\n",
      "|   |   |   |--- feature1 <= 6.05\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  6.05\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature4 >  1.7000000000000002\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "Testing Accuracy: 92.11\n"
     ]
    }
   ],
   "source": [
    "# Reads the data, splits it and fits a decision tree based on training data.\n",
    "# Accuracy is measured on testing data.\n",
    "data = pandas.read_csv(\"data/data.csv\")\n",
    "X = data.drop(\"class\", axis=1).values\n",
    "Y = data[\"class\"].values\n",
    "\n",
    "feature_names = list(data.drop(\"class\", axis=1).columns)\n",
    "class_names = [str(i) for i in range(0, len(set(Y)))]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "dt.print(feature_names, class_names)\n",
    "\n",
    "Y_test_pred = dt.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Decision Tree using Sklearn Library (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Decision Tree Classifier from the Sklearn Library and use gini index as a splitting measure. Use the data.csv dataset.\n",
    "Calculate accuracy for this model. \n",
    "Print the Decision tree and compare the Decision Trees generated from your code and Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature3 <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- feature3 >  2.45\n",
      "|   |--- feature3 <= 4.95\n",
      "|   |   |--- feature4 <= 1.70\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- feature4 >  1.70\n",
      "|   |   |   |--- feature2 <= 3.10\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |   |--- feature2 >  3.10\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |--- feature3 >  4.95\n",
      "|   |   |--- feature4 <= 1.70\n",
      "|   |   |   |--- feature1 <= 6.05\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature1 >  6.05\n",
      "|   |   |   |   |--- class: 2\n",
      "|   |   |--- feature4 >  1.70\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "\n",
      "Testing Accuracy: 94.74\n"
     ]
    }
   ],
   "source": [
    "# Reads the data, splits it and fits sklearn.tree.DecisionTreeClassifier based on training data.\n",
    "# Accuracy is measured on testing data.\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(export_text(clf, feature_names=feature_names))\n",
    "\n",
    "Y_test_pred = clf.predict(X_test)\n",
    "print()\n",
    "print(f'Testing Accuracy: {Utils.accuracy(Y_test, Y_test_pred) * 100:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision of the two generated decision trees:\n",
    "\n",
    "Both the trees generated are almost identical most of the time.\n",
    "The difference in the trees could occur because of the different splitting measure (Information Gain for my code and Gini for sklearn code). The difference is still minimal and the accuracy achieved by both of the trees is similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEBH56pukq_H"
   },
   "source": [
    "### Q2 Linear Regression (40 points)\n",
    "\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FL1tuQEkq_H"
   },
   "source": [
    "## Gradient descent algorithm \n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This minimizes the following cost function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\\begin{equation}\n",
    "h(x_i) = \\theta^T \\bar{x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cOem1EjQkq_H"
   },
   "outputs": [],
   "source": [
    "# Do not change the code in this cell\n",
    "true_slope = 15\n",
    "true_intercept = 2.4\n",
    "input_var = np.arange(0.0,100.0)\n",
    "output_var = true_slope * input_var + true_intercept + 300.0 * np.random.rand(len(input_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1630902228487,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "SNvDqYEykq_H",
    "outputId": "c7f53823-73d9-473f-9e28-944f1f09a415",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.scatter(input_var, output_var)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XgNhbpEmkq_I"
   },
   "outputs": [],
   "source": [
    "def compute_cost(ip, op, params):\n",
    "    \"\"\"\n",
    "    Cost function in linear regression where the cost is calculated\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    num_samples = len(ip)\n",
    "    cost_sum = 0.0\n",
    "    for x,y in zip(ip, op):\n",
    "        y_hat = np.dot(params, np.array([1.0, x]))\n",
    "        cost_sum += (y_hat - y) ** 2\n",
    "    \n",
    "    cost = cost_sum / (num_samples)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Implement Linear Regression using Batch Gradient Descent from scratch.  (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ao7aYu9kq_I"
   },
   "source": [
    "\n",
    "### Batch gradient descent\n",
    "Algorithm can be given as follows:\n",
    "\n",
    "```for j in 0 -> max_iteration: \n",
    "    for i in 0 -> m: \n",
    "        theta += (alpha / m) * (y[i] - h(x[i])) * x_bar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f0z876gDkq_I"
   },
   "outputs": [],
   "source": [
    "def linear_regression_using_batch_gradient_descent(ip, op, params, alpha, max_iter):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using batch gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    max_iter: maximum number of iterations\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\" \n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    iteration = 0\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(max_iter)\n",
    "    params_store = np.zeros([2, max_iter])\n",
    "\n",
    "    x = np.ones([num_samples, len(params)])\n",
    "    for i in range(num_samples):\n",
    "        x[i][1] = ip[i]\n",
    "    \n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    while iteration < max_iter:\n",
    "        cost[iteration] = compute_cost(ip, op, params)\n",
    "        params_store[:, iteration] = params\n",
    "        \n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {iteration}')\n",
    "        print(f'cost: {cost[iteration]}')\n",
    "\n",
    "\n",
    "        \n",
    "        # Apply batch gradient descent\n",
    "        for i in range(0, num_samples):\n",
    "            prevParams = params\n",
    "            for j in range(len(params)):\n",
    "                params[j] += (alpha / num_samples) * (op[i] - np.dot(prevParams, x[i])) * x[i][j]\n",
    "        iteration += 1\n",
    "    \n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qbjhyZ71kq_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13109487.09911338\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 21438.758957716\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 9754.638204553155\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 9809.046889591595\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 9809.937674548459\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 9808.635078823518\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 9807.261481261787\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 9805.886231995295\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 9804.511581562374\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 9803.137602673098\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 9801.764297368538\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 9800.391665396844\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 9799.019706431842\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 9797.64842014515\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 9796.277806208449\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 9794.907864293558\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 9793.538594072506\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 9792.169995217439\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 9790.80206740068\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 9789.43481029471\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 9788.068223572158\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 9786.702306905827\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 9785.337059968688\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 9783.97248243385\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 9782.608573974567\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 9781.24533426431\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 9779.882762976642\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 9778.520859785343\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 9777.15962436431\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 9775.799056387616\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 9774.439155529484\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 9773.07992146432\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 9771.721353866651\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 9770.363452411177\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 9769.006216772805\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 9767.649646626513\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 9766.293741647489\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 9764.938501511078\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 9763.583925892784\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 9762.230014468252\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 9760.876766913298\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 9759.524182903888\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 9758.172262116152\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 9756.821004226389\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 9755.47040891101\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 9754.120475846652\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 9752.771204710047\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 9751.422595178126\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 9750.074646927966\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 9748.727359636769\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 9747.380732981972\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 9746.034766641078\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 9744.689460291787\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 9743.344813611995\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 9742.000826279687\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 9740.657497973049\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 9739.314828370396\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 9737.972817150237\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 9736.631463991193\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 9735.290768572075\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 9733.950730571836\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 9732.611349669578\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 9731.272625544576\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 9729.934557876266\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 9728.59714634422\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 9727.26039062816\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 9725.924290408\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 9724.588845363778\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 9723.254055175703\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 9721.919919524127\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 9720.586438089571\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 9719.253610552698\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 9717.921436594343\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 9716.589915895496\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 9715.259048137283\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 9713.928833000984\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 9712.599270168064\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 9711.270359320139\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 9709.942100138946\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 9708.61449230639\n",
      "--------------------------\n",
      "iteration: 80\n",
      "cost: 9707.287535504565\n",
      "--------------------------\n",
      "iteration: 81\n",
      "cost: 9705.961229415672\n",
      "--------------------------\n",
      "iteration: 82\n",
      "cost: 9704.635573722097\n",
      "--------------------------\n",
      "iteration: 83\n",
      "cost: 9703.310568106379\n",
      "--------------------------\n",
      "iteration: 84\n",
      "cost: 9701.986212251202\n",
      "--------------------------\n",
      "iteration: 85\n",
      "cost: 9700.662505839384\n",
      "--------------------------\n",
      "iteration: 86\n",
      "cost: 9699.339448553968\n",
      "--------------------------\n",
      "iteration: 87\n",
      "cost: 9698.017040078066\n",
      "--------------------------\n",
      "iteration: 88\n",
      "cost: 9696.695280094991\n",
      "--------------------------\n",
      "iteration: 89\n",
      "cost: 9695.374168288201\n",
      "--------------------------\n",
      "iteration: 90\n",
      "cost: 9694.053704341311\n",
      "--------------------------\n",
      "iteration: 91\n",
      "cost: 9692.733887938102\n",
      "--------------------------\n",
      "iteration: 92\n",
      "cost: 9691.414718762466\n",
      "--------------------------\n",
      "iteration: 93\n",
      "cost: 9690.096196498493\n",
      "--------------------------\n",
      "iteration: 94\n",
      "cost: 9688.778320830419\n",
      "--------------------------\n",
      "iteration: 95\n",
      "cost: 9687.461091442618\n",
      "--------------------------\n",
      "iteration: 96\n",
      "cost: 9686.144508019632\n",
      "--------------------------\n",
      "iteration: 97\n",
      "cost: 9684.828570246149\n",
      "--------------------------\n",
      "iteration: 98\n",
      "cost: 9683.513277807007\n",
      "--------------------------\n",
      "iteration: 99\n",
      "cost: 9682.198630387207\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "# Training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n",
    "\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "\n",
    "alpha_batch = 1e-3\n",
    "max_iter = 100\n",
    "params_hat_batch, cost_batch, params_store_batch =\\\n",
    "    linear_regression_using_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Implement Stochastic Gradient Descent from scratch. (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEIJL-WGkq_I"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "Algorithm can be given as follows:\n",
    "```shuffle(x, y)\n",
    "for i in 0 -> m:\n",
    "    theta += (alpha / m) * (y[i] - h(x[i])) * x_bar  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Gx9LN0wQkq_I"
   },
   "outputs": [],
   "source": [
    "# I changed input_var, output_var to ip, op as it was not taking the parameter values but taking the input_var and output_var (the whole data) defined in the earlier block.\n",
    "def lin_reg_stoch_gradient_descent(ip, op, params, alpha):\n",
    "    \"\"\"\n",
    "    Compute the params for linear regression using stochastic gradient descent\n",
    "    ip: input variables\n",
    "    op: output variables\n",
    "    params: corresponding parameters\n",
    "    alpha: learning rate\n",
    "    Returns parameters, cost, params_store\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize iteration, number of samples, cost and parameter array\n",
    "    num_samples = len(ip)\n",
    "    cost = np.zeros(num_samples)\n",
    "    params_store = np.zeros([2, num_samples])\n",
    "\n",
    "    i = 0\n",
    "    # Compute the cost and store the params for the corresponding cost\n",
    "    for x, y in zip(ip, op):\n",
    "        cost[i] = compute_cost(ip, op, params)\n",
    "        params_store[:, i] = params\n",
    "\n",
    "        print('--------------------------')\n",
    "        print(f'iteration: {i}')\n",
    "        print(f'cost: {cost[i]}')\n",
    "\n",
    "        # Apply stochastic gradient descent\n",
    "        X = [1.0, x]\n",
    "        prevParams = params\n",
    "        for j in range(len(params)):\n",
    "            params[j] += (alpha / num_samples) * (y - np.dot(prevParams, X)) * X[j]\n",
    "        i+=1\n",
    "\n",
    "    return params, cost, params_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HivE1gVkkq_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "iteration: 0\n",
      "cost: 13109487.09911338\n",
      "--------------------------\n",
      "iteration: 1\n",
      "cost: 11028838.363106307\n",
      "--------------------------\n",
      "iteration: 2\n",
      "cost: 10596654.371537372\n",
      "--------------------------\n",
      "iteration: 3\n",
      "cost: 10118420.411211025\n",
      "--------------------------\n",
      "iteration: 4\n",
      "cost: 9511419.945653187\n",
      "--------------------------\n",
      "iteration: 5\n",
      "cost: 7848275.175178778\n",
      "--------------------------\n",
      "iteration: 6\n",
      "cost: 7183309.08645127\n",
      "--------------------------\n",
      "iteration: 7\n",
      "cost: 6986581.378929436\n",
      "--------------------------\n",
      "iteration: 8\n",
      "cost: 5401108.528232132\n",
      "--------------------------\n",
      "iteration: 9\n",
      "cost: 5274090.269320895\n",
      "--------------------------\n",
      "iteration: 10\n",
      "cost: 4824856.769693864\n",
      "--------------------------\n",
      "iteration: 11\n",
      "cost: 4439452.944778876\n",
      "--------------------------\n",
      "iteration: 12\n",
      "cost: 4358079.400813103\n",
      "--------------------------\n",
      "iteration: 13\n",
      "cost: 4334218.198118232\n",
      "--------------------------\n",
      "iteration: 14\n",
      "cost: 3988108.169239904\n",
      "--------------------------\n",
      "iteration: 15\n",
      "cost: 3823155.6069492223\n",
      "--------------------------\n",
      "iteration: 16\n",
      "cost: 3627035.0017695436\n",
      "--------------------------\n",
      "iteration: 17\n",
      "cost: 3287494.346117738\n",
      "--------------------------\n",
      "iteration: 18\n",
      "cost: 2973762.7293947404\n",
      "--------------------------\n",
      "iteration: 19\n",
      "cost: 2737147.7246587197\n",
      "--------------------------\n",
      "iteration: 20\n",
      "cost: 2570019.6652569254\n",
      "--------------------------\n",
      "iteration: 21\n",
      "cost: 2398516.1248558694\n",
      "--------------------------\n",
      "iteration: 22\n",
      "cost: 2395461.8774834354\n",
      "--------------------------\n",
      "iteration: 23\n",
      "cost: 2143907.3790928675\n",
      "--------------------------\n",
      "iteration: 24\n",
      "cost: 1873403.018984634\n",
      "--------------------------\n",
      "iteration: 25\n",
      "cost: 1597622.2382622946\n",
      "--------------------------\n",
      "iteration: 26\n",
      "cost: 1589109.6073656317\n",
      "--------------------------\n",
      "iteration: 27\n",
      "cost: 1331731.2014069625\n",
      "--------------------------\n",
      "iteration: 28\n",
      "cost: 1328079.9840383474\n",
      "--------------------------\n",
      "iteration: 29\n",
      "cost: 1323530.2734507145\n",
      "--------------------------\n",
      "iteration: 30\n",
      "cost: 1296348.4212695612\n",
      "--------------------------\n",
      "iteration: 31\n",
      "cost: 1164829.0988922634\n",
      "--------------------------\n",
      "iteration: 32\n",
      "cost: 1139560.2765501481\n",
      "--------------------------\n",
      "iteration: 33\n",
      "cost: 901626.6479966573\n",
      "--------------------------\n",
      "iteration: 34\n",
      "cost: 900238.9334733251\n",
      "--------------------------\n",
      "iteration: 35\n",
      "cost: 865933.234861932\n",
      "--------------------------\n",
      "iteration: 36\n",
      "cost: 865936.2268986527\n",
      "--------------------------\n",
      "iteration: 37\n",
      "cost: 708237.568649707\n",
      "--------------------------\n",
      "iteration: 38\n",
      "cost: 550583.9300007222\n",
      "--------------------------\n",
      "iteration: 39\n",
      "cost: 543519.8615650329\n",
      "--------------------------\n",
      "iteration: 40\n",
      "cost: 544327.2950874111\n",
      "--------------------------\n",
      "iteration: 41\n",
      "cost: 493268.92187541013\n",
      "--------------------------\n",
      "iteration: 42\n",
      "cost: 409278.5420983629\n",
      "--------------------------\n",
      "iteration: 43\n",
      "cost: 402081.52228526387\n",
      "--------------------------\n",
      "iteration: 44\n",
      "cost: 402056.7544726478\n",
      "--------------------------\n",
      "iteration: 45\n",
      "cost: 346349.9707961754\n",
      "--------------------------\n",
      "iteration: 46\n",
      "cost: 308651.36529755045\n",
      "--------------------------\n",
      "iteration: 47\n",
      "cost: 241973.39521898623\n",
      "--------------------------\n",
      "iteration: 48\n",
      "cost: 242046.17300407082\n",
      "--------------------------\n",
      "iteration: 49\n",
      "cost: 239617.52954192198\n",
      "--------------------------\n",
      "iteration: 50\n",
      "cost: 239089.30215234967\n",
      "--------------------------\n",
      "iteration: 51\n",
      "cost: 193722.34524543321\n",
      "--------------------------\n",
      "iteration: 52\n",
      "cost: 165199.4751146107\n",
      "--------------------------\n",
      "iteration: 53\n",
      "cost: 131806.48942668765\n",
      "--------------------------\n",
      "iteration: 54\n",
      "cost: 102868.29067232848\n",
      "--------------------------\n",
      "iteration: 55\n",
      "cost: 102186.7048725001\n",
      "--------------------------\n",
      "iteration: 56\n",
      "cost: 91385.44029240138\n",
      "--------------------------\n",
      "iteration: 57\n",
      "cost: 71096.51990670385\n",
      "--------------------------\n",
      "iteration: 58\n",
      "cost: 68342.4211139341\n",
      "--------------------------\n",
      "iteration: 59\n",
      "cost: 66930.78304805822\n",
      "--------------------------\n",
      "iteration: 60\n",
      "cost: 60363.65722643085\n",
      "--------------------------\n",
      "iteration: 61\n",
      "cost: 60888.673225419174\n",
      "--------------------------\n",
      "iteration: 62\n",
      "cost: 57944.85462949132\n",
      "--------------------------\n",
      "iteration: 63\n",
      "cost: 49764.617787615396\n",
      "--------------------------\n",
      "iteration: 64\n",
      "cost: 49138.36731116673\n",
      "--------------------------\n",
      "iteration: 65\n",
      "cost: 49062.09468967274\n",
      "--------------------------\n",
      "iteration: 66\n",
      "cost: 47333.01019274863\n",
      "--------------------------\n",
      "iteration: 67\n",
      "cost: 43849.130159650034\n",
      "--------------------------\n",
      "iteration: 68\n",
      "cost: 44484.83833926129\n",
      "--------------------------\n",
      "iteration: 69\n",
      "cost: 44695.75994366284\n",
      "--------------------------\n",
      "iteration: 70\n",
      "cost: 42142.99091545086\n",
      "--------------------------\n",
      "iteration: 71\n",
      "cost: 40484.366041763984\n",
      "--------------------------\n",
      "iteration: 72\n",
      "cost: 40747.605242736405\n",
      "--------------------------\n",
      "iteration: 73\n",
      "cost: 41473.534244515824\n",
      "--------------------------\n",
      "iteration: 74\n",
      "cost: 41814.02873273376\n",
      "--------------------------\n",
      "iteration: 75\n",
      "cost: 34339.77386739414\n",
      "--------------------------\n",
      "iteration: 76\n",
      "cost: 33959.85795665936\n",
      "--------------------------\n",
      "iteration: 77\n",
      "cost: 29493.474339014047\n",
      "--------------------------\n",
      "iteration: 78\n",
      "cost: 25594.056414260598\n",
      "--------------------------\n",
      "iteration: 79\n",
      "cost: 24662.951487816237\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "alpha = 1e-3\n",
    "params_0 = np.array([20.0, 80.0])\n",
    "params_hat, cost, params_store =\\\n",
    "lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error for Batch Gradient Descent: 88.71286727254743\n",
      "Root Mean Square Error for Stochastic Gradient Descent: 136.85336364091327\n"
     ]
    }
   ],
   "source": [
    "# Calculate Root Mean Square error in batch gradient descent algorithm and stochastic gradient descent algorithm\n",
    "def calcRMSE(X, Y, params):\n",
    "    \"\"\"\n",
    "    Calculates the Root mean square error.\n",
    "    :param X: The input varaible of samples\n",
    "    :param Y: The actual output variable of samples.\n",
    "    :param params: The parameters of linears regression\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    Y_cap = np.zeros(len(Y))\n",
    "    for i in range(0, len(Y)):\n",
    "        Y_cap[i] = np.dot(params, [1.0, X[i]])\n",
    "    E = Y - Y_cap\n",
    "    return np.sqrt(np.sum(E*E)/len(E))\n",
    "\n",
    "print(\"Root Mean Square Error for Batch Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat_batch)))\n",
    "print(\"Root Mean Square Error for Stochastic Gradient Descent: \"+ str(calcRMSE(x_test, y_test, params_hat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1630902274461,
     "user": {
      "displayName": "Praguna Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GheDZgFohozb1D4tdpw7nC3gfdiGtrrgzrUZufzqA=s64",
      "userId": "14769753629771591406"
     },
     "user_tz": 240
    },
    "id": "930loAL6kq_L",
    "outputId": "e807576e-9852-4857-9a29-d367f2e0b26b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min cost with BGD: 9682.198630387207\n",
      "min cost with SGD: 24662.951487816237\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "plt.figure()\n",
    "plt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\n",
    "plt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('normalized cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'min cost with BGD: {np.min(cost_batch)}')\n",
    "print(f'min cost with SGD: {np.min(cost)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrpju6Kwkq_N"
   },
   "source": [
    "### Q2.4 Which linear regression model do you think works best for this data? Explain in brief. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbTux39kq_N"
   },
   "source": [
    "Based on how data is created, we have:\n",
    "\\begin{equation}\n",
    "y = 15 x + 2.4 + 300.0 * uniform(0, 1)\n",
    "\\end{equation}\n",
    "Thus, the model that works best for this data would be the one that provides the average results.\n",
    "The expected value of $uniform(0, 1)$ is $0.5$. Thus we have:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y &= 15 x + 2.4 + 300.0 * E(uniform(0, 1))\\\\\n",
    "y &= 15 x + 2.4 + 300 * (0.5)\\\\\n",
    "y &= 152.4 + 15x\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This will fit best with data and hence the best linear regression model.\n",
    "Comparing it with $y = \\theta_0 + \\theta_1 x$, we have $\\theta_0 = 152.4$ and $\\theta_1 = 15$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p02LYZrkq_N"
   },
   "source": [
    "### Q3. Linear Regression Analytical Problem (10 points)\n",
    "Consider the following training data.\n",
    "\n",
    "| X1 | X2 | Y |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1.5 |\n",
    "| 1 | 0 | 2 |\n",
    "| 1 | 1 | 2.5 |\n",
    "Suppose the data comes from a model y = $_{0}$ +$_{1}$x1 +$_{2}$x2 for unknown constants $_{0}$,$_{1}$,$_{2}$. Use least squares linear regression to find an estimate of $_{0}$,$_{1}$,$_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have:\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "        1 & 0 & 0\\\\\n",
    "        1 & 0 & 1\\\\\n",
    "        1 & 1 & 0\\\\\n",
    "        1 & 1 & 1\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Y = \\begin{bmatrix}\n",
    "        0\\\\\n",
    "        1.5\\\\\n",
    "        2\\\\\n",
    "        2.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "X^T = \\begin{bmatrix}\n",
    "        1 & 1 & 1 & 1\\\\\n",
    "        0 & 0 & 1 & 1\\\\\n",
    "        0 & 1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence,\n",
    "\\begin{equation}\n",
    "X^TX = \\begin{bmatrix}\n",
    "        4 & 2 & 2\\\\\n",
    "        2 & 2 & 1\\\\\n",
    "        2 & 1 & 2\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "|X^TX| = 4(4-1) -2(4-2)+2(2-4)=4\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Cf(X^TX) = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "Adj(X^TX) = (Cf(X^TX))^T = \\begin{bmatrix}\n",
    "        3 & -2 & -2\\\\\n",
    "        -2 & 4 & 0\\\\\n",
    "        -2 & 0 & 4\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1} = \\frac{1}{|X^TX|} Adj(X^TX)= \\begin{bmatrix}\n",
    "        0.75 & -0.5 & -0.5\\\\\n",
    "        -0.5 & 1 & 0\\\\\n",
    "        -0.5 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "Now,\n",
    "\\begin{equation}\n",
    "(X^TX)^{-1}X^T = \\begin{bmatrix}\n",
    "        0.75 & 0.25 & 0.25 & -0.25\\\\\n",
    "        -0.5 & -0.5 & 0.5 & 0.5\\\\\n",
    "        -0.5 & 0.5 & -0.5 & 0.5\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We have, $\\theta^* = (X^TX)^{-1}X^TY$\n",
    "Thus,\n",
    "\\begin{equation}\n",
    "\\theta^* = \\begin{bmatrix}\n",
    "        \\theta_0\\\\\n",
    "        \\theta_1\\\\\n",
    "        \\theta_2\n",
    "    \\end{bmatrix}\n",
    "    =\\begin{bmatrix}\n",
    "        0.25\\\\\n",
    "        1.5\\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the linear regression equation becomes:\n",
    "\\begin{equation}\n",
    "y = 0.25 + 1.5X_1 + X_2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-1-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
